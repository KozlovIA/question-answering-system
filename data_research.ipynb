{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка документов и сохранение эмбеддингов в базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igorexy\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import arxiv\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Считывание тем из YAML ===\n",
    "def load_topics_from_yaml(file_path: str):\n",
    "    \"\"\"Считывает YAML файл и возвращает список тем.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Загрузка статей с arXiv ===\n",
    "def download_arxiv_papers(query: str, max_results: int, download_folder: str):\n",
    "    \"\"\"Загружает статьи с arXiv по запросу.\"\"\"\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        # sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    for result in search.results():\n",
    "        paper_id = result.entry_id.split('/')[-1]\n",
    "        pdf_path = os.path.join(download_folder, f'{paper_id}.pdf')\n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"Downloading {result.title}...\")\n",
    "            result.download_pdf(download_folder, f'{paper_id}.pdf')\n",
    "            print(f\"Saved to {pdf_path}\")\n",
    "        else:\n",
    "            print(f\"{result.title} already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Считывание текста из PDF ===\n",
    "def extract_text_from_pdf(file_path: str, extract_images=True):\n",
    "    \"\"\"Извлекает текст из PDF файла.\"\"\"\n",
    "    loader = PyPDFLoader(file_path, extract_images=extract_images)\n",
    "    pages = loader.load()\n",
    "    document_text = ' '.join([page.page_content for page in pages])\n",
    "    return document_text, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'rpA', 'x0': 16.34, 'x1': 36.34, 'top': 224.15999999999997, 'doctop': 224.15999999999997, 'bottom': 255.26, 'upright': False, 'height': 31.100000000000023, 'width': 20.000000000000004, 'direction': 'ttb'}\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf_columns(pdf_path):\n",
    "    text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # text += page.extract_text(x_tolerance=1, y_tolerance=1, layout=True) + \"\\n\"  # Коррекция разброса колонок\n",
    "            text += page.extract_words()\t\n",
    "    return text\n",
    "\n",
    "res = extract_text_from_pdf_columns(r\"E:\\ImportantFiles\\Documents\\University\\Magic App\\dataset_short\\Machine Learning\\clustering\\1004.0694v1.pdf\")\n",
    "print(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.count('\\f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Новая функция в надежде избавиться от лишних символов\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# def extract_text_from_pdf_fitz(pdf_path):\n",
    "#     doc = fitz.open(pdf_path)\n",
    "#     text = \"\"\n",
    "#     for page in doc:\n",
    "#         text += page.get_text()\n",
    "\n",
    "#     # Удаление служебных символов: переносы строк, лишние пробелы\n",
    "#     clean_text = ' '.join(text.split())\n",
    "#     return clean_text\n",
    "\n",
    "def extract_text_from_pdf_fitz(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        # Извлекаем только текст, игнорируя графику и другие объекты\n",
    "        text += page.get_text(\"text\")\n",
    "\n",
    "    # Удаление служебных символов\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_texts\u001b[39m(texts):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mencode(texts)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    return model.encode(texts).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Загрузка данных в ChromaDB ===\n",
    "def upload_to_chromadb(\n",
    "    documents: list,\n",
    "    collection_name: str,\n",
    "    db_path: str = \"./chroma_storage\",\n",
    "    embedding_function=None  # Функция эмбеддингов передается параметром\n",
    "):\n",
    "    \"\"\"Добавляет документы в коллекцию ChromaDB с поддержкой кастомных эмбеддингов.\"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    \n",
    "    # Создаем коллекцию с функцией эмбеддингов, если она передана\n",
    "    if embedding_function:\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "    else:\n",
    "        collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    for doc in documents:\n",
    "        data = {\n",
    "            \"ids\": [doc[\"ids\"]],\n",
    "            \"documents\": [doc[\"documents\"]],\n",
    "            \"metadatas\": [doc[\"metadata\"]],\n",
    "        }\n",
    "        # Если задана функция эмбеддингов, вычисляем и добавляем вектора\n",
    "        if embedding_function:\n",
    "            data[\"embeddings\"] = [embedding_function([doc[\"documents\"]])[0]]\n",
    "\n",
    "        collection.add(**data)\n",
    "    \n",
    "    print(f\"Uploaded {len(documents)} documents to collection '{collection_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    filename=\"log/error_arxiv.txt\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics: ['Machine Learning', 'Data Analysis', 'Optimization Techniques', 'Natural Language Processing', 'Computer Vision', 'Theoretical Foundations', 'Applied AI']\n"
     ]
    }
   ],
   "source": [
    "# Шаг 1. Считываем темы из YAML\n",
    "topics = load_topics_from_yaml('config/topics_short.yaml')\n",
    "print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Шаг 2. Загрузка статей по каждой теме\n",
    "max_results = 50\n",
    "\n",
    "\n",
    "for topic in tqdm(topics[6:]):\n",
    "    topic_name = topic['name']\n",
    "    folder = f\"dataset/{topic_name}\"#.replace(' ', '_')}\"\n",
    "\n",
    "    for keyword in topic['keywords']:\n",
    "        query = f\"all:\\\"{keyword}\\\"\"\n",
    "        try:\n",
    "            download_arxiv_papers(query, max_results=max_results, download_folder=folder + f'/{keyword}')\n",
    "        except Exception as e:\n",
    "            # Логирование ошибки\n",
    "            logging.error(f\"Ошибка при загрузке статей для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "            # (Необязательно) Вывод сообщения об ошибке в консоль\n",
    "            print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Модифицированное извлечение текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics: ['Machine Learning', 'Data Analysis', 'Optimization Techniques', 'Natural Language Processing', 'Computer Vision', 'Theoretical Foundations', 'Applied AI', 'Emerging Topics']\n"
     ]
    }
   ],
   "source": [
    "# Шаг 1. Считываем темы из YAML\n",
    "topics = load_topics_from_yaml('config/topics.yaml')\n",
    "print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Считывание текста из PDF ===\n",
    "import re\n",
    "\n",
    "def mode_extract_text_from_pdf(file_path: str):\n",
    "    \"\"\"Извлекает текст из PDF файла.\"\"\"\n",
    "\n",
    "    document_text = extract_text_from_pdf_fitz(file_path)\n",
    "\n",
    "    cleaned_page = document_text\n",
    "\n",
    "    # Удаляем спецсимволы, оставляя буквы, цифры, пробелы и пунктуацию\n",
    "    cleaned_page = re.sub(r'[^а-яА-ЯёЁa-zA-Z0-9\\s.,!?-]', '', cleaned_page)\n",
    "\n",
    "    # Убираем лишние переносы строк\n",
    "    cleaned_page = re.sub(r'\\n{2,}', '\\n', cleaned_page)\n",
    "\n",
    "    # Удаляем ссылки на литературу вида [1], [2], ...\n",
    "    cleaned_page = re.sub(r'\\[\\d+\\]', '', cleaned_page)\n",
    "\n",
    "    # Удаляем email-адреса\n",
    "    cleaned_page = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', cleaned_page)\n",
    "\n",
    "    # Удаляем артикулы на arXiv\n",
    "    cleaned_page = re.sub(r'arXiv:\\d+\\.\\d+(v\\d+)? \\[\\w+(\\.\\w+)?\\] \\d+ \\w+ \\d+', '', cleaned_page)\n",
    "\n",
    "    # Удаляем формулы, заключенные в $...$ или \\( ... \\)\n",
    "    cleaned_page = re.sub(r'\\$.*?\\$|\\\\\\(.*?\\\\\\)', '', cleaned_page)\n",
    "\n",
    "    # Удаляем числа\n",
    "    cleaned_page = re.sub(r'\\d+', '', cleaned_page)\n",
    "\n",
    "    # Удаляем математические символы и спецсимволы\n",
    "    cleaned_page = re.sub(r'[+\\-*/=<>^~_|&%$#@!]', '', cleaned_page)\n",
    "\n",
    "    # Удаляем гиперссылки\n",
    "    cleaned_page = re.sub(r'http\\S+', '', cleaned_page)\n",
    "\n",
    "    # Убираем лишние пробелы\n",
    "    cleaned_page = re.sub(r'\\s+', ' ', cleaned_page).strip()\n",
    "\n",
    "    # Проверяем наличие слова REFERENCES (регистр не учитывается)\n",
    "    if re.search(r'\\bREFERENCES\\b', cleaned_page, flags=re.IGNORECASE):\n",
    "        # Удаляем текст после \"REFERENCES\" (регистр не имеет значения)\n",
    "        cleaned_page = re.split(r'\\bREFERENCES\\b', cleaned_page, flags=re.IGNORECASE)[0]\n",
    "\n",
    "    cleaned_text = cleaned_page\n",
    "\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "# text = extract_text(r\"E:\\ImportantFiles\\Documents\\University\\Magic App\\dataset_short\\Machine Learning\\clustering\\1808.08317v1.pdf\")\n",
    "text = extract_text(r\"E:\\ImportantFiles\\Documents\\University\\Magic App\\dataset_short\\Data Analysis\\data visualization\\1705.01483v1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот метод не использовался в этом файле\n",
    "import re\n",
    "\n",
    "\n",
    "def replace_ligatures(text):\n",
    "    ligatures = {\n",
    "    \"ﬀ\": \"ff\", \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\",\n",
    "    \"Æ\": \"AE\", \"Œ\": \"OE\", \"Ǆ\": \"DZ\", \"ǅ\": \"Dz\",\n",
    "    \"Ϝ\": \"W\", \"Ϟ\": \"KS\",\n",
    "    \"Ꜳ\": \"AA\", \"ꜳ\": \"aa\", \"Ꜵ\": \"AO\"\n",
    "}\n",
    "\n",
    "    pattern = re.compile(\"|\".join(re.escape(k) for k in ligatures))\n",
    "    return pattern.sub(lambda m: ligatures[m.group()], text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = replace_ligatures(text)  # Удаляем лигатуры\n",
    "    text = re.sub(r'\\f', '', text)  # Удаляем символы \\f\n",
    "    text = re.sub(r'(?m)^.$', '', text)  # Удаляем строки с одним символом\n",
    "    text = re.sub(r'(?<![.!?])\\n(?!\\n)', ' ', text)  # Убираем лишние переносы строк\n",
    "    text = re.sub(r'(?<=\\w)-\\n', '', text)  # Убираем переносы слов\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Сводим подряд идущие переносы строк к одному\n",
    "    text = re.sub(r'\\bREFERENCES\\b.*', '', text, flags=re.IGNORECASE | re.DOTALL)  # Удаляем все после REFERENCES\n",
    "    text = re.sub(r'\\d{4,}.*', '', text)  # Удаляем непонятные числовые строки\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\.?\\s*$', '', text)  # Удаляем строки с номерами\n",
    "    text = re.sub(r'(?m)^([A-Za-z]+\\s*){1,3}\\d+$', '', text)  # Удаляем табличные данные\n",
    "    return text.strip()\n",
    "\n",
    "# Пример использования:\n",
    "raw_text = \"\"\"Clustering is an ubiquitous data analysis tool applied across diverse disciplines, such as\\n\n",
    "bioinformatics, marketing, and image segmentation. Its wide utility is perhaps unsurprising,\\n\n",
    "as its intuitive aim - to divide data into groups of similar items - applies at various stages of\\n\n",
    "modality tests, before delving into clusterability methods.\\n\n",
    "REFERENCES\\n1234 Some reference text that should be removed.\"\"\"\n",
    "\n",
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### извлечение по документам, без учета страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_short\\Machine Learning\\clustering\n",
      "dataset_short\\Machine Learning\\decision trees\n",
      "dataset_short\\Machine Learning\\deep learning\n",
      "dataset_short\\Machine Learning\\ensemble methods\n",
      "dataset_short\\Machine Learning\\neural networks\n",
      "dataset_short\\Machine Learning\\reinforcement learning\n",
      "MuPDF error: syntax error: could not parse color space (313 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (432 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (550 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (659 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (771 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (993 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (1289 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (1326 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (1601 0 R)\n",
      "\n",
      "dataset_short\\Machine Learning\\supervised learning\n",
      "dataset_short\\Machine Learning\\SVM\n",
      "dataset_short\\Machine Learning\\unsupervised learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 180 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Шаг 3. Извлечение текста из PDF\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for topic in tqdm([topics[0]]):\n",
    "# for topic in tqdm(topics[5:]):\n",
    "    topic_name = topic['name']\n",
    "    folder = rf\"dataset_short\\{topic_name}\"\n",
    "\n",
    "    for keyword in os.listdir(folder):\n",
    "        folder_keywords = os.path.join(folder, keyword)\n",
    "        print(folder_keywords)\n",
    "\n",
    "        for file_name in os.listdir(folder_keywords):\n",
    "            if file_name.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_keywords, file_name)\n",
    "\n",
    "                try:\n",
    "                    # Предполагается, что функция extract_text_from_pdf возвращает общий текст и список страниц\n",
    "                    document_text = mode_extract_text_from_pdf(file_path)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Ошибка при считывании текста для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "                    print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")\n",
    "                    continue\n",
    "\n",
    "                # Формируем записи для документа\n",
    "                all_documents.append({\n",
    "                    \"ids\": file_name.split('.pdf')[0],\n",
    "                    \"documents\": document_text,\n",
    "                    \"metadata\": {\n",
    "                        \"topic\": topic_name,\n",
    "                        \"keyword\": keyword,  # Добавляем ключевое слово в метаданные\n",
    "                        \"filename\": file_name,\n",
    "                    }\n",
    "                })\n",
    "\n",
    "print(f\"Extracted text from {len(all_documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В прошлый раз ушло 2 минуты на считывание только Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Шаг 4. Загрузка документов в ChromaDB\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mupload_to_chromadb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmagic_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mupload_to_chromadb\u001b[1;34m(documents, collection_name, db_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m collection \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_or_create_collection(name\u001b[38;5;241m=\u001b[39mcollection_name)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUploaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents to collection \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:81\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     48\u001b[0m     ids: OneOrMany[ID],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     59\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     add_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_add(\n\u001b[0;32m     91\u001b[0m         collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m     92\u001b[0m         ids\u001b[38;5;241m=\u001b[39madd_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[0;32m     99\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:90\u001b[0m, in \u001b[0;36mvalidation_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     92\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:213\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_add_request\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_records[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    212\u001b[0m     validate_record_set_for_embedding(record_set\u001b[38;5;241m=\u001b[39madd_records)\n\u001b[1;32m--> 213\u001b[0m     add_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_record_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     add_embeddings \u001b[38;5;241m=\u001b[39m add_records[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:526\u001b[0m, in \u001b[0;36mCollectionCommon._embed_record_set\u001b[1;34m(self, record_set, embeddable_fields)\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\n\u001b[0;32m    523\u001b[0m                 \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader(uris\u001b[38;5;241m=\u001b[39mcast(URIs, record_set[field]))  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[0;32m    524\u001b[0m             )\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 526\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecord_set\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[literal-required]\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecord does not contain any non-None fields that can be embedded.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddable Fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddable_fields\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecord Fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:539\u001b[0m, in \u001b[0;36mCollectionCommon._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/guides/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    538\u001b[0m     )\n\u001b[1;32m--> 539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\api\\types.py:460\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[1;32m--> 460\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_embeddings(cast(Embeddings, normalize_embeddings(result)))\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\onnx_mini_lm_l6_v2.py:200\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Documents) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# Only download the model when it is actually used\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_model_if_not_exists()\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Embeddings, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\chromadb\\utils\\embedding_functions\\onnx_mini_lm_l6_v2.py:143\u001b[0m, in \u001b[0;36mONNXMiniLM_L6_V2._forward\u001b[1;34m(self, documents, batch_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([e\u001b[38;5;241m.\u001b[39mattention_mask \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encoded])\n\u001b[0;32m    135\u001b[0m onnx_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(input_ids, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(attention_mask, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     ),\n\u001b[0;32m    142\u001b[0m }\n\u001b[1;32m--> 143\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Perform mean pooling with attention weighting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:220\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[0;32m    218\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Шаг 4. Загрузка документов в ChromaDB\n",
    "upload_to_chromadb(all_documents, collection_name=\"magic_document\", embedding_function=embed_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

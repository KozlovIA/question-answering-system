{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igorexy\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pdfminer.high_level import extract_text\n",
    "from llama_cpp import Llama\n",
    "import wordninja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all-MiniLM-L6-v2.yaml',\n",
       " 'all-mpnet-base-v2.yaml',\n",
       " 'distiluse-base-multilingual-cased-v1.yaml',\n",
       " 'e5-large-v2.yaml',\n",
       " 'gtr-t5-large.yaml',\n",
       " 'LaBSE.yaml',\n",
       " 'msmarco-distilbert-base-v4.yaml',\n",
       " 'multi-qa-MiniLM-L6-cos-v1.yaml',\n",
       " 'multi-qa-mpnet-base-dot-v1.yaml',\n",
       " 'multilingual-e5-large.yaml',\n",
       " 'paraphrase-multilingual-MiniLM-L12-v2.yaml',\n",
       " 'paraphrase-multilingual-mpnet-base-v2.yaml',\n",
       " 'questions_gen.yaml',\n",
       " 'stsb-xlm-r-multilingual.yaml']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files(directory):\n",
    "    return os.listdir(directory)\n",
    "\n",
    "# Пример использования:\n",
    "files_and_dirs = get_files(\"config/embedding/\")\n",
    "files_and_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_PATH = \"config/embedding/questions_gen.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/all-MiniLM-L6-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/all-mpnet-base-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/paraphrase-multilingual-MiniLM-L12-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/multi-qa-mpnet-base-dot-v1.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/LaBSE.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/distiluse-base-multilingual-cased-v1.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/msmarco-distilbert-base-v4.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/multi-qa-MiniLM-L6-cos-v1.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/paraphrase-multilingual-mpnet-base-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/stsb-xlm-r-multilingual.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/gtr-t5-large.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/e5-large-v2.yaml\"\n",
    "CONFIG_PATH = \"config/embedding/multilingual-e5-large.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Настройка логирования ===\n",
    "logging.basicConfig(\n",
    "    filename=\"log/error_arxiv.txt\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "with open(f\"{CONFIG_PATH}\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "CHROMA_PATH = config[\"chroma_path\"]\n",
    "COLLECTION_NAME = config[\"collection_name\"]\n",
    "MODEL_NAME = config.get(\"model_name\", None)\n",
    "# MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "MODEL_PATH = config.get(\"model_path\", r\"C:\\Users\\Igorexy\\.lmstudio\\models\\MaziyarPanahi\\Qwen2.5-7B-Instruct-GGUF\\Qwen2.5-7B-Instruct.Q4_K_S.gguf\")\n",
    "TOKEN_TRESHOLD = config.get(\"token_treshold\", 32768)\n",
    "\n",
    "DOCUMENTS_FOLDER = 'dataset'\n",
    "TOPIC_PATH = 'config/topic.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Загрузка модели эмбеддингов ===\n",
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_yaml(file_path: str):\n",
    "    \"\"\"Считывает YAML файл и возвращает список тем.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def replace_ligatures(text):\n",
    "    ligatures = {\n",
    "    \"ﬀ\": \"ff\", \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\",\n",
    "    \"Æ\": \"AE\", \"Œ\": \"OE\", \"Ǆ\": \"DZ\", \"ǅ\": \"Dz\",\n",
    "    \"Ϝ\": \"W\", \"Ϟ\": \"KS\",\n",
    "    \"Ꜳ\": \"AA\", \"ꜳ\": \"aa\", \"Ꜵ\": \"AO\"\n",
    "}\n",
    "\n",
    "    pattern = re.compile(\"|\".join(re.escape(k) for k in ligatures))\n",
    "    return pattern.sub(lambda m: ligatures[m.group()], text)\n",
    "\n",
    "\n",
    "def remove_after_last_references(text):\n",
    "    matches = list(re.finditer(r'\\bREFERENCES\\b', text, re.IGNORECASE))\n",
    "    \n",
    "    if matches:\n",
    "        last_match = matches[-1]  # Берём последнее вхождение REFERENCES\n",
    "        before_text = text[:last_match.start()]  # Текст до последнего REFERENCES\n",
    "        after_text = text[last_match.end():]  # Текст после последнего REFERENCES\n",
    "        \n",
    "        # Условие: удаляем текст после, если его меньше, чем до\n",
    "        if len(after_text) < len(before_text):\n",
    "            return before_text  # Возвращаем только текст до последнего REFERENCES\n",
    "        else:\n",
    "            return text  # Если после REFERENCES текста больше или равно, ничего не удаляем\n",
    "    \n",
    "    return text  # Если REFERENCES нет, возвращаем исходный текст\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = replace_ligatures(text)  # Удаляем лигатуры\n",
    "    text = re.sub(r'\\f', '', text)  # Удаляем символы \\f\n",
    "    text = remove_after_last_references(text)   # Удаление ссылок на литературу\n",
    "    text = re.sub(r'(?m)^.$', '', text)  # Удаляем строки с одним символом\n",
    "    text = re.sub(r'(?<![.!?])\\n(?!\\n)', ' ', text)  # Убираем лишние переносы строк\n",
    "    text = re.sub(r'(?<=\\w)-\\n', '', text)  # Убираем переносы слов\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Сводим подряд идущие переносы строк к одному\n",
    "    text = re.sub(r'\\d{4,}.*', '', text)  # Удаляем непонятные числовые строки\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\.?\\s*$', '', text)  # Удаляем строки с номерами\n",
    "    text = re.sub(r'(?m)^([A-Za-z]+\\s*){1,3}\\d+$', '', text)  # Удаляем табличные данные\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path: str):\n",
    "    \"\"\"Извлекает текст из PDF файла с очисткой.\"\"\"\n",
    "    text = extract_text(file_path)\n",
    "    restored_text = \" \".join(wordninja.split(text))     # восстановление пробелов\n",
    "    cleaned_text = clean_text(text)\n",
    "    return cleaned_text\n",
    "\n",
    "llm_token_check = Llama(model_path=MODEL_PATH, n_ctx=32768, verbose=False)\n",
    "def count_tokens_llama(text):\n",
    "    return len(llm_token_check.tokenize(text.encode(\"utf-8\"), add_bos=False))\n",
    "\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Создает эмбеддинги для списка текстов.\"\"\"\n",
    "    return model.encode(texts).tolist()\n",
    "\n",
    "def upload_to_chromadb(documents, collection_name, db_path=\"./chroma_storage\"):\n",
    "    \"\"\"Загружает документы в ChromaDB.\"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    for doc in documents:\n",
    "        data = {\n",
    "            \"ids\": [doc[\"ids\"]],\n",
    "            \"documents\": [doc[\"documents\"]],\n",
    "            \"metadatas\": [doc[\"metadata\"]],\n",
    "            \"embeddings\": [embed_texts([doc[\"documents\"]])[0]]\n",
    "        }\n",
    "        collection.add(**data)\n",
    "    \n",
    "    print(f\"Uploaded {len(documents)} documents to collection '{collection_name}'.\")\n",
    "\n",
    "\n",
    "def upload_unique_to_chromadb(documents, collection_name, db_path=\"./chroma_storage\"):\n",
    "    \"\"\"Добавляет в ChromaDB только новые документы, которых нет в базе.\"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    # Получаем список уже существующих ids в коллекции\n",
    "    existing_ids = set(collection.get()['ids'])\n",
    "    \n",
    "    new_documents = [doc for doc in documents if doc[\"ids\"] not in existing_ids]\n",
    "    \n",
    "    if not new_documents:\n",
    "        print(\"No new documents to upload.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"ids\": [doc[\"ids\"] for doc in new_documents],\n",
    "        \"documents\": [doc[\"documents\"] for doc in new_documents],\n",
    "        \"metadatas\": [doc[\"metadata\"] for doc in new_documents],\n",
    "        \"embeddings\": embed_texts([doc[\"documents\"] for doc in new_documents])\n",
    "    }\n",
    "    \n",
    "    collection.add(**data)\n",
    "    \n",
    "    print(f\"Uploaded {len(new_documents)} new documents to collection '{collection_name}'.\")\n",
    "\n",
    "def process_topic(topic, check_token=False, token_treshold=TOKEN_TRESHOLD):\n",
    "    \"\"\"Обрабатывает все документы по данной теме и загружает в ChromaDB.\"\"\"\n",
    "    topic_name = topic['name']\n",
    "    folder = rf\"{DOCUMENTS_FOLDER}\\{topic_name}\"\n",
    "    all_documents = []\n",
    "    files_to_delete = []\n",
    "    \n",
    "    for keyword in os.listdir(folder):\n",
    "        folder_keywords = os.path.join(folder, keyword)\n",
    "        print(folder_keywords)\n",
    "        \n",
    "        for file_name in os.listdir(folder_keywords):\n",
    "            if file_name.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_keywords, file_name)\n",
    "                try:\n",
    "                    document_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "                    if check_token:\n",
    "                        if count_tokens_llama(document_text) > token_treshold:\n",
    "                            files_to_delete.append(file_name)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Ошибка при считывании текста для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "                    print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Формируем записи для документа\n",
    "                all_documents.append({\n",
    "                    \"ids\": file_name.split('.pdf')[0],\n",
    "                    \"documents\": document_text,\n",
    "                    \"metadata\": {\n",
    "                        \"topic\": topic_name,\n",
    "                        \"keyword\": keyword,  # Добавляем ключевое слово в метаданные\n",
    "                        \"filename\": file_name,\n",
    "                    }\n",
    "                })\n",
    "    # Удаление файлов\n",
    "    # for file_path in files_to_delete:\n",
    "    #     try:\n",
    "    #         os.remove(file_path)\n",
    "    #         print(f\"Удалён: {file_path}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Ошибка при удалении {file_path}: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"Extracted text from {len(all_documents)} documents for topic '{topic_name}'\")\n",
    "    upload_to_chromadb(all_documents, collection_name=COLLECTION_NAME, db_path=CHROMA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск без сохранения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = load_from_yaml(TOPIC_PATH)['topics']\n",
    "# print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")\n",
    "\n",
    "# for topic in tqdm(topics):\n",
    "#     process_topic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант с process_topic без функции, с разделением на считывание текста и загрузку в векторную базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics: ['Machine Learning', 'Data Analysis']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\Machine Learning\\active learning\n",
      "dataset\\Machine Learning\\autoML\n",
      "dataset\\Machine Learning\\clustering\n",
      "dataset\\Machine Learning\\decision trees\n",
      "dataset\\Machine Learning\\deep learning\n",
      "dataset\\Machine Learning\\early stopping\n",
      "dataset\\Machine Learning\\ensemble methods\n",
      "dataset\\Machine Learning\\gradient boosting\n",
      "dataset\\Machine Learning\\hyperparameter tuning\n",
      "dataset\\Machine Learning\\learning rate\n",
      "dataset\\Machine Learning\\loss functions\n",
      "dataset\\Machine Learning\\model interpretability\n",
      "dataset\\Machine Learning\\model selection\n",
      "dataset\\Machine Learning\\neural networks\n",
      "dataset\\Machine Learning\\overfitting\n",
      "dataset\\Machine Learning\\reinforcement learning\n",
      "dataset\\Machine Learning\\supervised learning\n",
      "dataset\\Machine Learning\\SVM\n",
      "dataset\\Machine Learning\\transfer learning\n",
      "dataset\\Machine Learning\\underfitting\n",
      "dataset\\Machine Learning\\unsupervised learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [03:41<03:41, 221.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 220 documents for topic 'Machine Learning'\n",
      "dataset\\Data Analysis\\anova\n",
      "dataset\\Data Analysis\\correlation analysis\n",
      "dataset\\Data Analysis\\data aggregation\n",
      "dataset\\Data Analysis\\data preprocessing\n",
      "dataset\\Data Analysis\\data transformation\n",
      "dataset\\Data Analysis\\data visualization\n",
      "dataset\\Data Analysis\\descriptive statistics\n",
      "dataset\\Data Analysis\\dimensionality reduction\n",
      "dataset\\Data Analysis\\EDA\n",
      "dataset\\Data Analysis\\feature engineering\n",
      "dataset\\Data Analysis\\hypothesis testing\n",
      "dataset\\Data Analysis\\missing data imputation\n",
      "dataset\\Data Analysis\\normality tests\n",
      "dataset\\Data Analysis\\outlier detection\n",
      "Ошибка: ('Unhandled', 14) - при обработке темы 'Data Analysis', ключевого слова 'outlier detection'\n",
      "dataset\\Data Analysis\\regression analysis\n",
      "dataset\\Data Analysis\\sampling techniques\n",
      "dataset\\Data Analysis\\scaling and normalization\n",
      "dataset\\Data Analysis\\statistical analysis\n",
      "dataset\\Data Analysis\\t-test\n",
      "dataset\\Data Analysis\\time series analysis\n",
      "dataset\\Data Analysis\\z-test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [06:24<00:00, 192.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 436 documents for topic 'Data Analysis'\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topics = load_from_yaml(TOPIC_PATH)['topics']\n",
    "print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")\n",
    "\n",
    "DOCUMENTS_FOLDER = 'dataset'\n",
    "\n",
    "all_documents = []\n",
    "files_to_delete = []\n",
    "check_token = True\n",
    "token_treshold = TOKEN_TRESHOLD\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "\n",
    "    topic_name = topic['name']\n",
    "    folder = rf\"{DOCUMENTS_FOLDER}\\{topic_name}\"\n",
    "    if os.path.isdir(folder) == False:\n",
    "        print(f\"Папка {folder} не существует\")\n",
    "        continue\n",
    "\n",
    "    for keyword in os.listdir(folder):\n",
    "        folder_keywords = os.path.join(folder, keyword)\n",
    "        print(folder_keywords)\n",
    "        \n",
    "        for file_name in os.listdir(folder_keywords):\n",
    "            if file_name.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_keywords, file_name)\n",
    "                try:\n",
    "                    document_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "                    if check_token:     # Не записываем док, если превышает пороговое значение\n",
    "                        if count_tokens_llama(document_text) > token_treshold:\n",
    "                            files_to_delete.append(folder_keywords + \"\\\\\" + file_name)\n",
    "                            continue   \n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Ошибка при считывании текста для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "                    print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Формируем записи для документа\n",
    "                all_documents.append({\n",
    "                    \"ids\": file_name.split('.pdf')[0],\n",
    "                    \"documents\": document_text,\n",
    "                    \"metadata\": {\n",
    "                        \"topic\": topic_name,\n",
    "                        \"keyword\": keyword,  # Добавляем ключевое слово в метаданные\n",
    "                        \"filename\": file_name,\n",
    "                    }\n",
    "                })\n",
    "    print(f\"Extracted text from {len(all_documents)} documents for topic '{topic_name}'\")\n",
    "\n",
    "print(\"files_to_delete\", len(files_to_delete))\n",
    "# Удаление файлов\n",
    "# for file_path in files_to_delete:\n",
    "#     try:\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Удалён: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Ошибка при удалении {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалён: dataset\\Machine Learning\\active learning\\1906.05194v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\active learning\\2201.09433v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\active learning\\2211.14819v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\active learning\\2408.07364v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\autoML\\2007.04074v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\autoML\\2012.05390v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\autoML\\2302.10827v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\autoML\\2401.00379v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1004.0694v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1505.07872v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1506.01942v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1808.08317v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\2011.03720v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\2103.09329v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\2105.08348v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\1909.05299v5.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\1912.13132v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\2003.00617v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\2104.00673v4.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\2206.08841v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\2306.06591v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\cross-validation\\2406.01950v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2108.03887v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2405.15911v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2503.01455v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1805.08355v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1812.05448v4.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1901.09388v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\2303.01980v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2108.05574v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2212.03462v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2301.11556v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2409.06830v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2502.04709v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\early stopping\\2502.13283v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1206.4645v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1408.1336v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1909.05303v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\2209.13369v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\2302.02097v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\gradient boosting\\1803.02042v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\gradient boosting\\1906.10991v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\1903.05176v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\1907.00036v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\2101.06427v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\2105.14625v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\2307.10262v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\hyperparameter tuning\\2503.23595v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\1907.04595v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\2002.09647v4.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\2102.08716v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\2110.10710v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\2110.12634v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\learning rate\\2412.15745v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\loss functions\\1808.06733v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\loss functions\\2112.05927v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\loss functions\\2301.05579v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\loss functions\\2306.15368v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\loss functions\\2501.09924v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\model interpretability\\0412015v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\model interpretability\\2006.05379v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\model interpretability\\2305.14395v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\model interpretability\\2407.11359v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\model selection\\0809.3092v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\model selection\\1208.0129v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\model selection\\1307.2307v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\model selection\\1612.08490v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\model selection\\2004.07583v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\2304.05133v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\2408.04747v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\overfitting\\2101.00914v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\overfitting\\2202.06526v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\overfitting\\2209.13382v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\overfitting\\2401.10359v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\overfitting\\2412.00560v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2108.03258v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2108.11510v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2204.05437v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2212.00253v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2307.01452v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2308.11336v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1905.11590v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2103.00845v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2206.00845v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2501.14148v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1008.4000v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1201.4714v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1309.3877v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1609.09162v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1611.07659v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\2102.04849v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\transfer learning\\1708.05629v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\transfer learning\\1911.02685v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\transfer learning\\2202.03070v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\transfer learning\\2206.10593v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\underfitting\\1802.10582v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\underfitting\\2010.06076v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\underfitting\\2305.02139v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\underfitting\\2410.16901v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1906.02826v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1911.02344v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\0504499v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\1911.05580v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\2110.04849v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\2301.13177v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\2408.00973v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\2408.12319v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\anova\\2502.15215v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\1111.6308v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\1209.3761v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\1304.7981v5.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\1705.10865v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\1808.00685v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\correlation analysis\\2103.00361v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data aggregation\\1806.04874v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data aggregation\\2303.11641v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1810.06021v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1912.09722v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2111.14120v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2308.10915v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2407.00005v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2409.14912v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data transformation\\1803.00701v4.pdf\n",
      "Удалён: dataset\\Data Analysis\\data transformation\\2006.04410v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data transformation\\2209.13981v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data transformation\\2306.14320v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data transformation\\2309.12168v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1901.01920v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2408.04386v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2412.16402v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\descriptive statistics\\0704.1847v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\descriptive statistics\\1201.6601v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\descriptive statistics\\2102.01144v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\0709.2773v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\0807.4424v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1811.12199v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1902.08571v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\2105.13773v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\2211.16752v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\EDA\\1111.2221v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\EDA\\1503.01954v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\EDA\\2502.10857v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2110.11592v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2301.03532v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2404.04959v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2404.16870v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2503.14434v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\hypothesis testing\\1609.07528v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\hypothesis testing\\1809.04329v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\hypothesis testing\\2012.00077v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\missing data imputation\\2011.02089v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\missing data imputation\\2110.12002v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\missing data imputation\\2203.05089v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\missing data imputation\\2211.00783v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\normality tests\\1901.07851v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\normality tests\\1907.01736v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\normality tests\\2109.08427v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\outlier detection\\1907.13276v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\outlier detection\\2005.09900v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\outlier detection\\2106.05127v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\outlier detection\\2108.00360v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\outlier detection\\2502.05496v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1403.0060v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\2205.07804v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\2207.04082v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\sampling techniques\\1704.06835v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\sampling techniques\\2208.09619v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\sampling techniques\\2309.10658v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\sampling techniques\\2311.12646v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\sampling techniques\\2406.15832v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\0509559v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\1311.3409v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\1411.2235v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\1601.02463v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\2110.07029v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\scaling and normalization\\2208.00603v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\0902.0408v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\2502.08114v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\t-test\\2007.07065v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\2403.14735v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\2503.07674v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\z-test\\1601.03640v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\z-test\\1704.07865v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\z-test\\1803.05384v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\z-test\\1810.08054v3.pdf\n"
     ]
    }
   ],
   "source": [
    "# Удаление файлов\n",
    "for file_path in files_to_delete:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Удалён: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при удалении {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset\\\\Machine Learning\\\\cross-validation\\\\1909.05299v5.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\1912.13132v1.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\2003.00617v2.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\2104.00673v4.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\2206.08841v1.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\2306.06591v2.pdf',\n",
       " 'dataset\\\\Machine Learning\\\\cross-validation\\\\2406.01950v1.pdf']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload в БД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 436 documents to collection 'multilingual-e5-large-embedding'.\n"
     ]
    }
   ],
   "source": [
    "upload_to_chromadb(all_documents, collection_name=COLLECTION_NAME, db_path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

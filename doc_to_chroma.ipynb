{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Igorexy\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\keras\\losses.py:2664: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pdfminer.high_level import extract_text\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all-MiniLM-L6-v2.yaml',\n",
       " 'all-mpnet-base-v2.yaml',\n",
       " 'multi-qa-mpnet-base-dot-v1.yaml',\n",
       " 'paraphrase-multilingual-MiniLM-L12-v2.yaml',\n",
       " 'questions_gen.yaml']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_files(directory):\n",
    "    return os.listdir(directory)\n",
    "\n",
    "# Пример использования:\n",
    "files_and_dirs = get_files(\"config/embedding/\")\n",
    "files_and_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG_PATH = \"config/embedding/questions_gen.yaml\"\n",
    "CONFIG_PATH = \"config/embedding/all-MiniLM-L6-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/all-mpnet-base-v2.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/multi-qa-mpnet-base-dot-v1.yaml\"\n",
    "# CONFIG_PATH = \"config/embedding/paraphrase-multilingual-MiniLM-L12-v2.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Настройка логирования ===\n",
    "logging.basicConfig(\n",
    "    filename=\"log/error_arxiv.txt\",\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "with open(f\"{CONFIG_PATH}\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "CHROMA_PATH = config[\"chroma_path\"]\n",
    "COLLECTION_NAME = config[\"collection_name\"]\n",
    "MODEL_NAME = config.get(\"model_name\", None)\n",
    "# MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "MODEL_PATH = config.get(\"model_path\", r\"C:\\Users\\Igorexy\\.lmstudio\\models\\MaziyarPanahi\\Qwen2.5-7B-Instruct-GGUF\\Qwen2.5-7B-Instruct.Q4_K_S.gguf\")\n",
    "TOKEN_TRESHOLD = config.get(\"token_treshold\", 32768)\n",
    "\n",
    "DOCUMENTS_FOLDER = 'dataset'\n",
    "TOPIC_PATH = 'config/topics_short.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Загрузка модели эмбеддингов ===\n",
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_yaml(file_path: str):\n",
    "    \"\"\"Считывает YAML файл и возвращает список тем.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def replace_ligatures(text):\n",
    "    ligatures = {\n",
    "    \"ﬀ\": \"ff\", \"ﬁ\": \"fi\", \"ﬂ\": \"fl\", \"ﬃ\": \"ffi\", \"ﬄ\": \"ffl\", \"ﬅ\": \"ft\", \"ﬆ\": \"st\",\n",
    "    \"Æ\": \"AE\", \"Œ\": \"OE\", \"Ǆ\": \"DZ\", \"ǅ\": \"Dz\",\n",
    "    \"Ϝ\": \"W\", \"Ϟ\": \"KS\",\n",
    "    \"Ꜳ\": \"AA\", \"ꜳ\": \"aa\", \"Ꜵ\": \"AO\"\n",
    "}\n",
    "\n",
    "    pattern = re.compile(\"|\".join(re.escape(k) for k in ligatures))\n",
    "    return pattern.sub(lambda m: ligatures[m.group()], text)\n",
    "\n",
    "\n",
    "def remove_after_last_references(text):\n",
    "    matches = list(re.finditer(r'\\bREFERENCES\\b', text, re.IGNORECASE))\n",
    "    \n",
    "    if matches:\n",
    "        last_match = matches[-1]  # Берём последнее вхождение REFERENCES\n",
    "        before_text = text[:last_match.start()]  # Текст до последнего REFERENCES\n",
    "        after_text = text[last_match.end():]  # Текст после последнего REFERENCES\n",
    "        \n",
    "        # Условие: удаляем текст после, если его меньше, чем до\n",
    "        if len(after_text) < len(before_text):\n",
    "            return before_text  # Возвращаем только текст до последнего REFERENCES\n",
    "        else:\n",
    "            return text  # Если после REFERENCES текста больше или равно, ничего не удаляем\n",
    "    \n",
    "    return text  # Если REFERENCES нет, возвращаем исходный текст\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = replace_ligatures(text)  # Удаляем лигатуры\n",
    "    text = re.sub(r'\\f', '', text)  # Удаляем символы \\f\n",
    "    text = remove_after_last_references(text)   # Удаление ссылок на литературу\n",
    "    text = re.sub(r'(?m)^.$', '', text)  # Удаляем строки с одним символом\n",
    "    text = re.sub(r'(?<![.!?])\\n(?!\\n)', ' ', text)  # Убираем лишние переносы строк\n",
    "    text = re.sub(r'(?<=\\w)-\\n', '', text)  # Убираем переносы слов\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)  # Сводим подряд идущие переносы строк к одному\n",
    "    text = re.sub(r'\\d{4,}.*', '', text)  # Удаляем непонятные числовые строки\n",
    "    text = re.sub(r'(?m)^\\s*\\d+\\.?\\s*$', '', text)  # Удаляем строки с номерами\n",
    "    text = re.sub(r'(?m)^([A-Za-z]+\\s*){1,3}\\d+$', '', text)  # Удаляем табличные данные\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path: str):\n",
    "    \"\"\"Извлекает текст из PDF файла с очисткой.\"\"\"\n",
    "    text = extract_text(file_path)\n",
    "    cleaned_text = clean_text(text)\n",
    "    return cleaned_text\n",
    "\n",
    "llm_token_check = Llama(model_path=MODEL_PATH, n_ctx=32768, verbose=False)\n",
    "def count_tokens_llama(text):\n",
    "    return len(llm_token_check.tokenize(text.encode(\"utf-8\"), add_bos=False))\n",
    "\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Создает эмбеддинги для списка текстов.\"\"\"\n",
    "    return model.encode(texts).tolist()\n",
    "\n",
    "def upload_to_chromadb(documents, collection_name, db_path=\"./chroma_storage\"):\n",
    "    \"\"\"Загружает документы в ChromaDB.\"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    for doc in documents:\n",
    "        data = {\n",
    "            \"ids\": [doc[\"ids\"]],\n",
    "            \"documents\": [doc[\"documents\"]],\n",
    "            \"metadatas\": [doc[\"metadata\"]],\n",
    "            \"embeddings\": [embed_texts([doc[\"documents\"]])[0]]\n",
    "        }\n",
    "        collection.add(**data)\n",
    "    \n",
    "    print(f\"Uploaded {len(documents)} documents to collection '{collection_name}'.\")\n",
    "\n",
    "\n",
    "def upload_unique_to_chromadb(documents, collection_name, db_path=\"./chroma_storage\"):\n",
    "    \"\"\"Добавляет в ChromaDB только новые документы, которых нет в базе.\"\"\"\n",
    "    client = chromadb.PersistentClient(path=db_path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    # Получаем список уже существующих ids в коллекции\n",
    "    existing_ids = set(collection.get()['ids'])\n",
    "    \n",
    "    new_documents = [doc for doc in documents if doc[\"ids\"] not in existing_ids]\n",
    "    \n",
    "    if not new_documents:\n",
    "        print(\"No new documents to upload.\")\n",
    "        return\n",
    "    \n",
    "    data = {\n",
    "        \"ids\": [doc[\"ids\"] for doc in new_documents],\n",
    "        \"documents\": [doc[\"documents\"] for doc in new_documents],\n",
    "        \"metadatas\": [doc[\"metadata\"] for doc in new_documents],\n",
    "        \"embeddings\": embed_texts([doc[\"documents\"] for doc in new_documents])\n",
    "    }\n",
    "    \n",
    "    collection.add(**data)\n",
    "    \n",
    "    print(f\"Uploaded {len(new_documents)} new documents to collection '{collection_name}'.\")\n",
    "\n",
    "def process_topic(topic, check_token=False, token_treshold=TOKEN_TRESHOLD):\n",
    "    \"\"\"Обрабатывает все документы по данной теме и загружает в ChromaDB.\"\"\"\n",
    "    topic_name = topic['name']\n",
    "    folder = rf\"{DOCUMENTS_FOLDER}\\{topic_name}\"\n",
    "    all_documents = []\n",
    "    files_to_delete = []\n",
    "    \n",
    "    for keyword in os.listdir(folder):\n",
    "        folder_keywords = os.path.join(folder, keyword)\n",
    "        print(folder_keywords)\n",
    "        \n",
    "        for file_name in os.listdir(folder_keywords):\n",
    "            if file_name.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_keywords, file_name)\n",
    "                try:\n",
    "                    document_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "                    if check_token:\n",
    "                        if count_tokens_llama(document_text) > token_treshold:\n",
    "                            files_to_delete.append(file_name)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Ошибка при считывании текста для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "                    print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Формируем записи для документа\n",
    "                all_documents.append({\n",
    "                    \"ids\": file_name.split('.pdf')[0],\n",
    "                    \"documents\": document_text,\n",
    "                    \"metadata\": {\n",
    "                        \"topic\": topic_name,\n",
    "                        \"keyword\": keyword,  # Добавляем ключевое слово в метаданные\n",
    "                        \"filename\": file_name,\n",
    "                    }\n",
    "                })\n",
    "    # Удаление файлов\n",
    "    # for file_path in files_to_delete:\n",
    "    #     try:\n",
    "    #         os.remove(file_path)\n",
    "    #         print(f\"Удалён: {file_path}\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Ошибка при удалении {file_path}: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"Extracted text from {len(all_documents)} documents for topic '{topic_name}'\")\n",
    "    upload_to_chromadb(all_documents, collection_name=COLLECTION_NAME, db_path=CHROMA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4908"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = extract_text_from_pdf(r\"E:\\ImportantFiles\\Documents\\University\\Magic App\\dataset\\Machine Learning\\clustering\\0105522v1.pdf\")\n",
    "\n",
    "count_tokens_llama(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск без сохранения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics: ['Machine Learning', 'Data Analysis', 'Optimization Techniques', 'Natural Language Processing', 'Computer Vision', 'Theoretical Foundations', 'Applied AI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_short\\Machine Learning\\clustering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded topics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[topic[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mtopic\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtopics]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m tqdm(topics):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mprocess_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m, in \u001b[0;36mprocess_topic\u001b[1;34m(topic, check_token, token_treshold)\u001b[0m\n\u001b[0;32m     96\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_keywords, file_name)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     document_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_token:\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m count_tokens_llama(document_text) \u001b[38;5;241m>\u001b[39m token_treshold:\n",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_from_pdf\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Извлекает текст из PDF файла с очисткой.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\high_level.py:175\u001b[0m, in \u001b[0;36mextract_text\u001b[1;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[0;32m    166\u001b[0m interpreter \u001b[38;5;241m=\u001b[39m PDFPageInterpreter(rsrcmgr, device)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m PDFPage\u001b[38;5;241m.\u001b[39mget_pages(\n\u001b[0;32m    169\u001b[0m     fp,\n\u001b[0;32m    170\u001b[0m     page_numbers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m     caching\u001b[38;5;241m=\u001b[39mcaching,\n\u001b[0;32m    174\u001b[0m ):\n\u001b[1;32m--> 175\u001b[0m     \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_string\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfinterp.py:997\u001b[0m, in \u001b[0;36mPDFPageInterpreter.process_page\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m    995\u001b[0m     ctm \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39mx0, \u001b[38;5;241m-\u001b[39my0)\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mbegin_page(page, ctm)\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mend_page(page)\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfinterp.py:1016\u001b[0m, in \u001b[0;36mPDFPageInterpreter.render_contents\u001b[1;34m(self, resources, streams, ctm)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_resources(resources)\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(ctm)\n\u001b[1;32m-> 1016\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfinterp.py:1042\u001b[0m, in \u001b[0;36mPDFPageInterpreter.execute\u001b[1;34m(self, streams)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name, args)\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m==\u001b[39m nargs:\n\u001b[1;32m-> 1042\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1044\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexec: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfinterp.py:902\u001b[0m, in \u001b[0;36mPDFPageInterpreter.do_TJ\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mncs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 902\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_string\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPDFTextSeq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphicstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfdevice.py:133\u001b[0m, in \u001b[0;36mPDFTextDevice.render_string\u001b[1;34m(self, textstate, seq, ncs, graphicstate)\u001b[0m\n\u001b[0;32m    118\u001b[0m     textstate\u001b[38;5;241m.\u001b[39mlinematrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_string_vertical(\n\u001b[0;32m    119\u001b[0m         seq,\n\u001b[0;32m    120\u001b[0m         matrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         graphicstate,\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     textstate\u001b[38;5;241m.\u001b[39mlinematrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_string_horizontal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtextstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinematrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfontsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcharspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwordspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdxscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mncs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraphicstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\pdfdevice.py:173\u001b[0m, in \u001b[0;36mPDFTextDevice.render_string_horizontal\u001b[1;34m(self, seq, matrix, pos, font, fontsize, scaling, charspace, wordspace, rise, dxscale, ncs, graphicstate)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needcharspace:\n\u001b[0;32m    172\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m charspace\n\u001b[1;32m--> 173\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_char\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfont\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfontsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mncs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraphicstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cid \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m wordspace:\n\u001b[0;32m    184\u001b[0m     x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m wordspace\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\converter.py:250\u001b[0m, in \u001b[0;36mPDFLayoutAnalyzer.render_char\u001b[1;34m(self, matrix, font, fontsize, scaling, rise, cid, ncs, graphicstate)\u001b[0m\n\u001b[0;32m    237\u001b[0m textdisp \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mchar_disp(cid)\n\u001b[0;32m    238\u001b[0m item \u001b[38;5;241m=\u001b[39m LTChar(\n\u001b[0;32m    239\u001b[0m     matrix,\n\u001b[0;32m    240\u001b[0m     font,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m     graphicstate,\n\u001b[0;32m    249\u001b[0m )\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcur_item\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m item\u001b[38;5;241m.\u001b[39madv\n",
      "File \u001b[1;32mc:\\Users\\Igorexy\\anaconda3\\envs\\science_assistant\\Lib\\site-packages\\pdfminer\\layout.py:447\u001b[0m, in \u001b[0;36mLTContainer.add\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objs)\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: LTItemT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objs\u001b[38;5;241m.\u001b[39mappend(obj)\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "topics = load_from_yaml(TOPIC_PATH)['topics']\n",
    "print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "    process_topic(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариант с process_topic без функции, с разделением на считывание текста и загрузку в векторную базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded topics: ['Machine Learning', 'Data Analysis', 'Optimization Techniques', 'Natural Language Processing', 'Computer Vision', 'Theoretical Foundations', 'Applied AI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\\Machine Learning\\clustering\n",
      "dataset\\Machine Learning\\decision trees\n",
      "dataset\\Machine Learning\\deep learning\n",
      "dataset\\Machine Learning\\ensemble methods\n",
      "dataset\\Machine Learning\\neural networks\n",
      "dataset\\Machine Learning\\reinforcement learning\n",
      "dataset\\Machine Learning\\supervised learning\n",
      "dataset\\Machine Learning\\SVM\n",
      "dataset\\Machine Learning\\unsupervised learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [01:10<07:05, 70.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 107 documents for topic 'Machine Learning'\n",
      "dataset\\Data Analysis\\data preprocessing\n",
      "dataset\\Data Analysis\\data visualization\n",
      "dataset\\Data Analysis\\dimensionality reduction\n",
      "dataset\\Data Analysis\\feature engineering\n",
      "dataset\\Data Analysis\\regression analysis\n",
      "dataset\\Data Analysis\\statistical analysis\n",
      "dataset\\Data Analysis\\time series analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [02:07<05:11, 62.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 194 documents for topic 'Data Analysis'\n",
      "dataset\\Optimization Techniques\\bayesian optimization\n",
      "dataset\\Optimization Techniques\\convex optimization\n",
      "dataset\\Optimization Techniques\\evolutionary algorithms\n",
      "dataset\\Optimization Techniques\\genetic algorithms\n",
      "dataset\\Optimization Techniques\\gradient descent\n",
      "dataset\\Optimization Techniques\\linear programming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3/7 [02:52<03:37, 54.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 275 documents for topic 'Optimization Techniques'\n",
      "dataset\\Natural Language Processing\\language modeling\n",
      "dataset\\Natural Language Processing\\sentiment analysis\n",
      "dataset\\Natural Language Processing\\sequence-to-sequence models\n",
      "dataset\\Natural Language Processing\\text mining\n",
      "dataset\\Natural Language Processing\\topic modeling\n",
      "dataset\\Natural Language Processing\\transformers\n",
      "dataset\\Natural Language Processing\\word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4/7 [03:25<02:18, 46.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 354 documents for topic 'Natural Language Processing'\n",
      "dataset\\Computer Vision\\convolutional neural networks\n",
      "dataset\\Computer Vision\\generative adversarial networks\n",
      "dataset\\Computer Vision\\image classification\n",
      "dataset\\Computer Vision\\image segmentation\n",
      "dataset\\Computer Vision\\object detection\n",
      "dataset\\Computer Vision\\transfer learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5/7 [03:55<01:20, 40.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 423 documents for topic 'Computer Vision'\n",
      "dataset\\Theoretical Foundations\\Bayesian inference\n",
      "dataset\\Theoretical Foundations\\complexity theory\n",
      "dataset\\Theoretical Foundations\\information theory\n",
      "dataset\\Theoretical Foundations\\probability theory\n",
      "dataset\\Theoretical Foundations\\statistical learning theory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6/7 [04:27<00:37, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 483 documents for topic 'Theoretical Foundations'\n",
      "dataset\\Applied AI\\AI ethics\n",
      "dataset\\Applied AI\\anomaly detection\n",
      "dataset\\Applied AI\\autonomous systems\n",
      "dataset\\Applied AI\\forecasting models\n",
      "dataset\\Applied AI\\recommender systems\n",
      "dataset\\Applied AI\\robotics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [05:24<00:00, 46.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from 551 documents for topic 'Applied AI'\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topics = load_from_yaml(TOPIC_PATH)['topics']\n",
    "print(f\"Loaded topics: {[topic['name'] for topic in topics]}\")\n",
    "\n",
    "DOCUMENTS_FOLDER = 'dataset'\n",
    "TOPIC_PATH = 'config/topics_short.yaml'\n",
    "\n",
    "all_documents = []\n",
    "files_to_delete = []\n",
    "check_token = True\n",
    "token_treshold = TOKEN_TRESHOLD\n",
    "\n",
    "for topic in tqdm(topics):\n",
    "\n",
    "    topic_name = topic['name']\n",
    "    folder = rf\"{DOCUMENTS_FOLDER}\\{topic_name}\"\n",
    "    if os.path.isdir(folder) == False:\n",
    "        print(f\"Папка {folder} не существует\")\n",
    "        continue\n",
    "\n",
    "    for keyword in os.listdir(folder):\n",
    "        folder_keywords = os.path.join(folder, keyword)\n",
    "        print(folder_keywords)\n",
    "        \n",
    "        for file_name in os.listdir(folder_keywords):\n",
    "            if file_name.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_keywords, file_name)\n",
    "                try:\n",
    "                    document_text = extract_text_from_pdf(file_path)\n",
    "\n",
    "                    if check_token:     # Не записываем док, если превышает пороговое значение\n",
    "                        if count_tokens_llama(document_text) > token_treshold:\n",
    "                            files_to_delete.append(folder_keywords + \"\\\\\" + file_name)\n",
    "                            continue   \n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Ошибка при считывании текста для темы '{topic_name}', ключевого слова '{keyword}': {e}\")\n",
    "                    print(f\"Ошибка: {e} - при обработке темы '{topic_name}', ключевого слова '{keyword}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Формируем записи для документа\n",
    "                all_documents.append({\n",
    "                    \"ids\": file_name.split('.pdf')[0],\n",
    "                    \"documents\": document_text,\n",
    "                    \"metadata\": {\n",
    "                        \"topic\": topic_name,\n",
    "                        \"keyword\": keyword,  # Добавляем ключевое слово в метаданные\n",
    "                        \"filename\": file_name,\n",
    "                    }\n",
    "                })\n",
    "    print(f\"Extracted text from {len(all_documents)} documents for topic '{topic_name}'\")\n",
    "\n",
    "print(files_to_delete)\n",
    "# Удаление файлов\n",
    "# for file_path in files_to_delete:\n",
    "#     try:\n",
    "#         os.remove(file_path)\n",
    "#         print(f\"Удалён: {file_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Ошибка при удалении {file_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалён: dataset\\Machine Learning\\clustering\\0306145v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\0906.2145v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1004.0694v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1209.4257v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1307.4838v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1412.2601v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1503.02059v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\clustering\\1506.06327v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\1004.0436v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\1206.4620v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\1805.08328v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\1909.13488v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2003.04952v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2006.14118v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2010.06631v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2012.08735v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\decision trees\\2108.03887v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1711.03577v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1801.00631v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1802.00810v4.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1802.08717v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1808.09772v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1812.05448v4.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1901.09388v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1906.01388v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1906.06706v7.pdf\n",
      "Удалён: dataset\\Machine Learning\\deep learning\\1908.08843v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\0906.0881v5.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\0909.3593v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1206.4645v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1408.1336v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1502.01733v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1604.07554v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1703.10936v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1902.07855v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\ensemble methods\\1909.05303v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\1312.2853v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\1610.01439v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\1809.09645v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\2006.00690v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\2102.13221v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\neural networks\\2106.13594v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\1708.05866v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2011.13577v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2108.03258v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2204.05437v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2205.09550v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2206.01233v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2212.00253v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2304.10098v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2306.05810v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\reinforcement learning\\2307.01452v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1110.3109v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1204.3965v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1811.02986v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1902.01449v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1905.03670v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1905.11590v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\1912.01096v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2001.03260v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2002.00763v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2103.00845v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\supervised learning\\2103.13559v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1008.4000v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1201.4714v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1309.3877v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1311.0914v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1504.05035v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1505.05451v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1507.03229v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1508.02479v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1608.01026v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1609.09162v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1611.07659v2.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1702.08019v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1812.02261v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1902.05731v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1903.04297v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\SVM\\1910.05329v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1011.1379v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1605.09131v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1610.01132v3.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1808.04593v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1901.07288v1.pdf\n",
      "Удалён: dataset\\Machine Learning\\unsupervised learning\\1906.02826v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1107.5472v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1810.06021v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1901.09376v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\1912.09722v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2202.12440v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2206.04875v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2305.11181v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2407.00005v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data preprocessing\\2409.14912v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1209.1125v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1901.01920v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1903.06671v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1907.05609v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1908.00192v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1908.11344v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1909.13322v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\1911.03871v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2107.01887v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2107.12547v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2108.07855v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\data visualization\\2211.03296v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\0301167v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\0405271v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\0703034v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1002.1156v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1106.2325v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1305.4345v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1412.2404v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1508.00636v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1511.00831v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1605.09093v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1707.04281v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1801.09052v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\dimensionality reduction\\1801.09390v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\1712.06778v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\1804.07814v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\1804.09770v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\1909.01185v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2003.02556v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2007.11455v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2009.02557v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2010.08784v3.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2105.11319v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\feature engineering\\2109.07604v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1011.5810v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1303.4728v4.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1403.0060v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1503.04407v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1511.05728v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1511.05925v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1603.00974v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1702.03994v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1708.05609v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1710.01523v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\regression analysis\\1801.03238v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\0107244v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\0203023v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\0703291v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\0902.0408v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\1002.4682v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\statistical analysis\\1102.5549v4.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\0702814v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1304.1209v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1310.1304v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1709.08055v2.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1806.08946v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1902.09425v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1905.07848v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\1906.00055v1.pdf\n",
      "Удалён: dataset\\Data Analysis\\time series analysis\\2205.01138v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1501.04080v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1604.01348v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1709.05501v6.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1807.02811v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1905.02370v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\1911.01032v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\bayesian optimization\\2002.01569v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\0705.4253v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1412.3297v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1501.01497v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1502.06192v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1603.05643v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1710.10329v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1805.01656v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1901.10682v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\convex optimization\\1905.06263v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\0810.2055v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1202.1708v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1207.0578v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1301.0929v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1403.0943v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1404.3520v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1504.06363v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\evolutionary algorithms\\1603.06788v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\genetic algorithms\\0803.2957v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\genetic algorithms\\0809.1613v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\genetic algorithms\\0903.2805v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\genetic algorithms\\1209.1048v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\genetic algorithms\\1606.04306v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\1507.02030v3.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\1903.03614v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\1907.11746v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\2001.09126v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\2001.11897v3.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\2101.02397v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\2102.02396v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\gradient descent\\2103.04902v2.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1204.0957v3.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1212.2953v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1303.7041v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1406.3466v1.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1507.00228v5.pdf\n",
      "Удалён: dataset\\Optimization Techniques\\linear programming\\1510.04823v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\language modeling\\1810.12387v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\language modeling\\2103.06495v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\language modeling\\2104.12277v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\language modeling\\2203.05936v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\topic modeling\\1203.3462v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\topic modeling\\1511.03020v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\topic modeling\\1702.07186v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\topic modeling\\1803.11045v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\topic modeling\\1808.03733v2.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\transformers\\0912.5462v6.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\transformers\\1310.2279v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\transformers\\1403.2188v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\transformers\\1504.06106v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\transformers\\1605.08683v1.pdf\n",
      "Удалён: dataset\\Natural Language Processing\\word embeddings\\2001.09876v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1509.09308v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1611.04358v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1710.00974v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1803.02129v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1904.08755v4.pdf\n",
      "Удалён: dataset\\Computer Vision\\convolutional neural networks\\1905.02473v5.pdf\n",
      "Удалён: dataset\\Computer Vision\\generative adversarial networks\\1701.04862v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\generative adversarial networks\\1706.04987v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\generative adversarial networks\\1710.10916v3.pdf\n",
      "Удалён: dataset\\Computer Vision\\generative adversarial networks\\1804.08641v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\generative adversarial networks\\1805.11202v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\image classification\\1807.05206v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\image classification\\2105.08149v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\image classification\\2312.01232v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\image segmentation\\1611.09811v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\image segmentation\\2203.05898v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\image segmentation\\2405.01857v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\image segmentation\\2502.18550v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\1711.11577v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\1803.05549v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\1809.06065v3.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\1905.04740v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\1912.01844v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\2011.02298v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\object detection\\2012.12397v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\1307.2312v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\1708.05629v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\1903.11020v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\1905.13672v2.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\1911.07489v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\2009.01989v1.pdf\n",
      "Удалён: dataset\\Computer Vision\\transfer learning\\2102.01530v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\0110093v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1012.3584v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1203.0617v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1205.6658v5.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1304.4333v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1711.01125v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1805.03700v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1902.06886v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\1903.00186v4.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\Bayesian inference\\2101.04468v3.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\complexity theory\\1210.8368v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\complexity theory\\1309.1801v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\complexity theory\\1501.03022v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\information theory\\0808.2837v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\information theory\\1002.1446v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\information theory\\1107.2984v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\0110253v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\0202077v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\0607217v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\0706.0750v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\0911.0087v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1207.4117v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1302.5325v4.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1502.01048v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1602.08867v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1605.06689v4.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\probability theory\\1608.00141v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\0610451v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1211.4860v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1305.4952v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1602.05866v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1610.04210v2.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1710.10600v1.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1802.07426v3.pdf\n",
      "Удалён: dataset\\Theoretical Foundations\\statistical learning theory\\1810.10465v2.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\1906.06668v2.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2004.08377v2.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2007.00700v1.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2011.12750v2.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2102.09364v1.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2102.12406v1.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2105.02407v1.pdf\n",
      "Удалён: dataset\\Applied AI\\AI ethics\\2111.09478v1.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2003.01412v3.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2006.09654v1.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2106.08570v1.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2107.13353v1.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2210.07548v1.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2211.13968v2.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2302.06670v3.pdf\n",
      "Удалён: dataset\\Applied AI\\anomaly detection\\2302.11239v3.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\1806.00693v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\1806.09608v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\1810.02144v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\1811.10277v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\1911.07133v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2004.04227v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2009.00738v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2012.00856v3.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2302.03778v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2302.10087v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2303.09579v1.pdf\n",
      "Удалён: dataset\\Applied AI\\autonomous systems\\2304.14335v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\1202.6487v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\1610.02653v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\1702.03613v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\1903.04827v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\1904.10959v2.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2101.03298v2.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2105.01098v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2111.14728v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2210.02447v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2211.04976v1.pdf\n",
      "Удалён: dataset\\Applied AI\\forecasting models\\2303.09405v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\1012.0498v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\1811.11866v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\1812.05095v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2007.04782v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2101.12153v2.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2102.12413v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2105.12353v2.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2109.08045v1.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2110.06287v2.pdf\n",
      "Удалён: dataset\\Applied AI\\recommender systems\\2208.09864v1.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1403.2625v1.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1408.2072v1.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1610.04080v2.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1701.07790v2.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1804.06383v1.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1812.06784v4.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1904.03049v2.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1909.05777v1.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\1909.10496v2.pdf\n",
      "Удалён: dataset\\Applied AI\\robotics\\2005.07474v1.pdf\n"
     ]
    }
   ],
   "source": [
    "# Удаление файлов\n",
    "for file_path in files_to_delete:\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        print(f\"Удалён: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при удалении {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload в БД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 551 documents to collection 'all-MiniLM-L6-v2-embedding'.\n"
     ]
    }
   ],
   "source": [
    "upload_to_chromadb(all_documents, collection_name=COLLECTION_NAME, db_path=CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght_list = [len(doc['documents']) for doc in all_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27606.734831460675"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(lenght_list).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': '0012536v1',\n",
       " 'documents': 'Draft version April 26, \\n OPTICAL AND X-RAY CLUSTERS AS TRACERS OF THE SUPERCLUSTER-VOID NETWORK.\\nI SUPERCLUSTERS OF ABELL AND X-RAY CLUSTERS M. Einasto1, J. Einasto 1, E. Tago1, V. M¨uller 2 & H. Andernach3 Draft version April 26, \\n ABSTRACT\\n We study the distribution of X-ray selected clusters of galaxies with respect to superclusters determined by Abell clusters of galaxies and show that the distribution of X-ray clusters follows the supercluster-void network determined by Abell clusters. We find that in this network X-ray clusters are more strongly clustered than other clusters: the fraction of X-ray clusters is higher in rich superclusters, and the fraction of isolated X-ray clusters is lower than the fraction of isolated Abell clusters. There is no clear correlation between X-ray luminosity of clusters and their host supercluster richness. Poor, non-Abell X-ray clusters follow the supercluster-void network as well: these clusters are embedded in superclusters determined by rich clusters and populate filaments between them. We present a new catalog of superclusters of Abell clusters out to a redshift of zlim = 0.13, a catalog of X-ray clusters located in superclusters determined by Abell clusters, and a list of additional superclusters of X-ray clusters.\\nSubject headings: cosmology: large-scale structure of the universe – cosmology: observations – galaxies:\\n X-ray clusters – galaxies: clusters\\n\\n INTRODUCTION\\n The formation of a filamentary web of galaxies and sys- tems of galaxies is predicted in any physically motivated model of structure formation in the Universe (Bond, Kof- man and Pogosyan \\n The fine structure of superclusters with their galaxy and cluster chains and filaments, and voids in-between, is presently quite well studied. The structure of the supercluster-void network itself is known with much less accuracy. Recently Einasto et al. (\\n An independent line of evidence for the structure of the Universe on large scales comes from the analysis of the CMB angular spectrum (de Bernardis et al. \\n So far superclusters have been determined using rich clus- ters of galaxies from the catalogs by Abell (\\n ple using certain probabilities which represent various se- lection effects. The influence of these selection effects can be studied by comparison of samples of clusters of galaxies selected independently. One of these optically selected in- dependent cluster samples is the catalog of clusters derived from scans with the Automated Plate Measuring (APM) Facility (Dalton et al. \\nUsually these studies analyze the correlation function on scales up to about 100 h−1 Mpc (Romer et al. \\n Another approach is to compile catalogs of superclusters of galaxies and to study the distribution of clusters in super- clusters. Supercluster catalogues have been used for many purposes – to investigate the distribution of high-density regions in the Universe, the large-scale motions in the Uni- verse, the analysis of the Sunyaev-Zeldovich effect (the scat- tering of the cosmic microwave background radiation by hot gas in clusters and superclusters of galaxies) in cosmic mi- crowave background maps. Examples of the last type of analyses are Birkinshaw (\\n The main goal of this series of papers is to compare the distribution of Abell, X-ray selected and APM clusters of galaxies and to check how well these cluster samples trace the properties of the underlying true cluster distribution and the supercluster-void network. We present an updated version of the supercluster catalog based on Abell clusters, supercluster catalogs of X-ray and APM clusters, and a list of X-ray clusters in superclusters determined by Abell clus- ters. We compare the distribution of Abell, X-ray and APM clusters in different environments. The aim of this analysis is twofold: it gives us information about the clustering prop- erties of Abell, X-ray and APM clusters; and independent evidence about how well different cluster samples trace the distribution of high-density regions of the Universe. In the first paper of the series (this Paper) we compare clustering properties of Abell and X-ray selected clusters in superclus- ters. In paper II we shall analyze the correlation function of X-ray clusters and provide evidence for a characteristic scale of 120h−1 Mpc in the distribution of X-ray clusters (Tago et al. \\n The paper is organized as follows. In Section 2 we shall describe cluster samples used and present an updated ver sion of the catalog of superclusters of Abell clusters.\\nIn Section 3 we compile a list of X-ray clusters in superclus- ters, analyze the distribution of Abell and non-Abell clus- ters, calculate the fraction of X-ray clusters in superclusters of different richness, and look for a relation between X-ray luminosities of clusters with the richness of their parent su- perclusters. In Section 4 we draw our conclusions. In the Appendix we present an updated version of the supercluster catalog based on Abell clusters, and a list of X-ray clusters in superclusters and in additional systems not present in the supercluster catalog. The catalog and both lists are also available electronically at web pages of Tartu Observatory (www.aai.ee). There we also demonstrate 3-D computer models and animations of the distribution of superclusters and X-ray clusters.\\n 2. DATA\\n 2.1. Abell clusters\\n For the present study we shall use the latest version (March \\n used. In the case of superimposed clusters or component clusters (A,B,C etc) with comparable number of measured redshifts, we used only the cluster which better matches the estimated redshift.\\n log(zmeas/zest)\\n Distances to clusters have been calculated using the fol lowing formula (Mattig \\n r =\\n H0q2\\n q0z + (q0\\n 1)(√1 + 2q0z 1 + z\\n 1)\\n (1)\\n where c is the velocity of light; H0 – the Hubble pa- rameter; and q0 – the deceleration parameter. We use H0 = 100 h−1 km s−1 Mpc−1, and q0 = 0.5.\\n 2.2. Superclusters of Abell clusters\\n On the basis of the Abell cluster sample we constructed a list of superclusters of Abell clusters using a friends-of- friends (FoF) algorithm described in detail by EETDA and E97c. Clusters are assigned to superclusters using a certain neighborhood radius so that all clusters in the system have at least one neighbor at a distance not exceeding this radius.\\n Fig. 1.— Left panel: The multiplicity functions for Abell clusters. The solid line shows the fraction of isolated clusters as function of the neighborhood radius R; the short-dashed line shows the fraction of clusters in medium-rich systems with a number of members from 2 to 31.\\nThe dashed line shows the fraction of clusters in very rich systems with at least 32 member clusters. Right panel: Supercluster multiplicities for a neighborhood radius R = 24 h−1 Mpc. Isolated clusters are included for comparison.\\n The neighborhood radius to assign clusters to superclusters should be chosen in accordance with the spatial density of the cluster sample. Also, we define the multiplicity of a supercluster (supercluster richness), NCL, as the number of its member clusters. Superclusters are divided into richness classes as in E97c: poor superclusters (number of members NCL = 2, 3), rich superclusters (4 7), and very rich superclusters (NCL\\n NCL\\n 8).\\n NCL\\n In Figure 1 (left panel) we show the fraction of clusters in systems of different multiplicity for a wide range of neigh- borhood radii for the Abell cluster sample. At small radii all clusters are isolated. With increasing neighborhood radius some clusters form superclusters of intermediate richness.\\nIn Figure 1 we plot the fraction of clusters in superclus- 31. At larger radii extremely ters of richness 2 large superclusters with multiplicity NCL 32 start to form. By further increasing the neighborhood radius su- perclusters begin to merge into huge conglomerates; finally all clusters percolate and form a single system penetrating the whole space under study. In order to obtain superclus- ters as the largest still relatively isolated systems we must choose a neighborhood radius smaller than the percolation radius. The appropriate neighborhood radius is the radius which corresponds to the maximum of the fraction of clus- ters in systems of intermediate richness. Beyond this radius very large systems start to form, as seen from Figure 1 (see also EETDA and E97c). For Abell clusters the appropriate neighborhood radius to select systems is 24 h−1 Mpc. We shall apply the same radius to the samples of X-ray clusters in order to determine which non-Abell X-ray clusters are the members of superclusters of Abell clusters, as well as to detect additional superclusters of non-Abell X-ray clusters.\\nFor the present study we update the supercluster cata- log and determine systems up to redshifts z = 0.13. This larger redshift limit was used in order to include several distant rich superclusters whose members have measured\\n redshifts and which also contain X-ray clusters, e.g.\\nthe Draco-Ursa Majoris supercluster with 14 member clusters.\\nThe new Abell supercluster catalog contains 285 superclus- ters with at least 2 member clusters, 31 of them are very rich superclusters with at least 8 members. The catalog of superclusters of Abell clusters is given in the Appendix (Table A1). In Figure 1 (right panel) we plot supercluster multiplicities for this catalog. In the present study this su- percluster catalog was used as a reference to look for X-ray clusters in superclusters.\\n 2.3. X-ray selected cluster samples\\n The ROSAT observations were made with the Position Sensitive Proportional Counter during the ROSAT All-sky Survey (RASS) in \\n On the basis of RASS data several catalogs of X-ray se- lected clusters of galaxies were prepared.\\nIn the present paper we shall use the following samples of X-ray clusters: i) clusters from the all-sky ROSAT Bright Survey of high Galactic latitude RASS sources. A detailed description of the data is given in Voges et al. \\n ii) ROSAT PSPC observations of the richest (R\\n 2) ACO clusters (David, Forman and Jones \\n iii) a flux-limited sample of bright clusters from the Southern sky (de Grandi et al. \\n iv) the ROSAT brightest cluster sample (Ebeling et\\n al. \\n Redshifts are available for all the clusters.\\nThe ROSAT Bright Survey is the only available all\\n sky survey of X-ray clusters. Objects in this survey > 30◦, with have been selected at Galactic latitudes, PSPC count rate larger than 0.2 s−1 and flux limit 2.4\\n 10−12 erg cm−2 s−1 in the hard energy band (0.5 2.0 keV).\\nFor our analysis we selected clusters with measured red- shifts up to z = 0.13 – the redshift limit of the catalog of superclusters of Abell clusters (see above). Altogether, this sample comprises 203 clusters, including 40 non-Abell clus- ters. We shall refer to this cluster sample as the “RBSC” sample; for cluster numbers we use RBS numbers as given in Schwope et al. (\\n Further, we use the list of the richest (R\\n 2) Abell clus- ters detected with ROSAT PSPC observations (DFJ). This catalog contains data on the clusters of galaxies observed during the GO phase of the ROSAT mission. The main ad- vantage of these observations is longer exposure time (typi- cally 10 000 seconds) than in the RASS (400 seconds). How- ever, the sky coverage of this compilation is far less than that of RBSC catalog since the latter clusters were found in targeted and serendipitous observations. For the method to calculate X-ray fluxes we refer to DFJ. Up to distances z = 0.13 this sample contains 52 clusters. We shall denote this sample as DFJ.\\n > 20◦ in the broad energy band (0.1\\n The Brightest Cluster Sample (BCS, Ebeling et al. \\n flux limit for sample was 4.4 10−12ergs cm−2s−1. Ebeling et al. developed the VTP (Voronoi Tessellation and Per- colation) algorithm to determine X-ray fluxes of extended sources of arbitrary shapes. Up to z = 0.13 this sample contains 141 clusters, including 46 non-Abell clusters. We shall denote this sample as BCS.\\n The flux-limited sample of bright clusters of galaxies from the Southern sky by de Grandi et al. (\\n and the flux limit in the hard band (0.5 2.0 keV) was 10−12 erg cm−2 s−1. In their study the so-called\\n Steepness Ratio Technique was used to determine X-ray fluxes. Up to z = 0.13 this sample contains 101 clusters, 34 of which are non-Abell clusters.\\n We shall discuss the completeness and selection effects of Abell and X-ray clusters in Paper II. In general, at distances larger than approximately 250 h−1 Mpc the samples of X- ray clusters are rather diluted due to the fixed flux limit; on larger distances X-ray clusters have been used in the present paper for lists of supercluster members only (and not for correlation analysis in Paper II).\\n 3. X-RAY CLUSTERS IN SUPERCLUSTERS\\n In this Section we compile a list of X-ray clusters that belong to the superclusters derived from Abell clusters as In addition, we searched for systems listed in Table A1.\\nconsisting of non-Abell X-ray clusters and determine their location with respect to the supercluster-void network. We also calculate the fraction of X-ray clusters in superclusters of various richness and investigate the possible correlation between cluster X-ray luminosities and supercluster rich- nesses.\\n 3.1. A list of X-ray clusters in superclusters\\n In Table B1 we present a list of X-ray clusters in su- perclusters of Abell clusters presented in Table A1. Abell clusters from X-ray catalogs were included by comparison of\\n the catalogs of X-ray clusters with the supercluster catalog.\\nIn order to include non-Abell X-ray clusters we searched for superclusters that contain X-ray clusters in two ways.\\nFirst, we added non-Abell X-ray clusters to our Abell clus- ter catalog and applied the FoF algorithm to this combined catalog. Second, we applied the FoF algorithm to each cat- In both cases we used alog of X-ray clusters separately.\\nthe same neighborhood radius, R = 24 h−1 Mpc as in the case of Abell clusters. The second procedure was used to check whether X-ray clusters that are supercluster members form systems by themselves also. Additionally, for some superclusters this second procedure detects outlying Abell clusters as members of superclusters that are not listed in Table A1 (mainly due to small differences in redshift mea- surements). In the case of X-ray clusters identified as Abell clusters this double procedure gives us additional evidence about the reliability of the superclusters found by optical surveys.\\n Non-Abell clusters that were found to be members of su- perclusters of Abell clusters (Table A1) were considered as members of these systems. However, their membership has to be checked carefully. The superclusters of Abell clusters were defined as the largest still relatively isolated systems.\\nIn some cases non-Abell clusters (poor clusters of galaxies) really belong to the superclusters, but in other cases non- Abell clusters actually form a bridge of poor clusters that connect superclusters of Abell clusters. Therefore, the ac- tual location of each non-Abell cluster that was connected to some supercluster according to the FoF algorithm was checked separately. We shall mention below the cases when clusters formed filaments connecting superclusters, rather than forming new members of a single supercluster.\\n We note that in most cases when a supercluster con- tains more than one X-ray cluster, these X-ray clusters themselves form a supercluster at the neighborhood radius R = 24 h−1 Mpc. Therefore Table B1 lists superclusters of X-ray clusters as well. Only in a few cases of very elon- gated superclusters it happened that some X-ray members of the system remained as separate systems so that the su- percluster was split into smaller systems. The supercluster number in the column 1 of Table B1 correspond to super- cluster numbers from the catalog in Table A1.\\n The use of combined (X-ray and optical) data to deter- mine X-ray clusters in superclusters was very fruitful. In our catalog of superclusters containing X-ray clusters (Ta- ble B1) there are 99 superclusters. Of these superclusters 53 contain only one member as an X-ray cluster. These X- ray clusters would be isolated if we would use data on X-ray clusters only; actually they are members of superclusters.\\nSuch an approach could be useful in the analysis of systems of X-ray selected AGNs, as mentioned also in Tesch and Engels (\\n In Table B2 we list additional superclusters that contain non-Abell clusters. In most cases these systems are pairs of Abell and non-Abell X-ray clusters. Most Abell clusters in these superclusters were isolated if only Abell clusters were used in supercluster search. We shall denote these superclusters as SCLX + supercluster number from Table B2.\\n 3.2. Comments on individual superclusters\\n The Hercules supercluster (SCL 160) at a distance of about 100 h−1 Mpc contains the largest number of X-ray\\n Y [h−1 Mpc] \\n\\n\\n\\n\\n\\n\\n −100\\n −200\\n Y [h−1 Mpc] \\n −300\\n −300\\n −200\\n −100\\n\\n\\n\\n −100\\n −200\\n\\n\\n\\n −100\\n −200\\n\\n\\n\\n −300\\n −300\\n −200\\n −100\\n\\n\\n\\n −100\\n −200\\n −300\\n −300\\n −200\\n −100\\n Y [h−1 Mpc] \\n\\n\\n\\n −300\\n −300\\n −200\\n −100\\n Y [h−1 Mpc] \\n\\n\\n\\n Fig. 2.— The distribution of X-ray clusters (filled symbols, supercluster members) and Abell clusters (open circles) in supergalactic coor- dinates. In order to avoid overcrowding of the figure we plot only clusters from very rich superclusters) in supergalactic coordinates. In each panel we plot Abell clusters and X-ray clusters from one sample. X-ray samples are plotted as follows. Upper left panel: RBS sample. Here we plot also members of additional systems (squares, Table B2), and isolated non-Abell clusters (triangles); upper right panel: DF J sample; lower left panel: BCS sample, and lower right panel: sample by de Grandi et al. (\\n clusters – 14, including 7 non-Abell clusters. All of them are probably true supercluster members.\\n The Shapley supercluster (SCL 124) at a distance of about 130 h−1 Mpc contains 9 X-ray clusters, only one of them is a non-Abell cluster. In this supercluster X-ray emis- sion has been detected also from filaments of galaxies con- necting individual clusters (Bardelli et al. \\nThe Horologium-Reticulum supercluster (SCL 48), one of the richest superclusters in the Southern sky, is also very rich in X-ray clusters, containing 11 X-ray clusters; only one of them is a non-Abell cluster. We note that the number of optically very rich X-ray clusters from the compilation by DFJ is the largest in the last two superclusters, in the Shap ley and in the Horologium-Reticulum superclusters, both containing six X-ray clusters.\\n The supercluster SCL 170 is very interesting. Accord- ing to the data used in our study this supercluster contains only one X-ray cluster – A\\n Fraction of X-ray clusters in superclusters of different richness\\n Table 1\\n Supercluster richness\\n NA\\n NX−ray\\n NA\\n FA NnA FnA\\n scl members 3) poor (2\\n rich (4 7)\\n very rich (Ncl > 8)\\n Ncl Ncl\\n \\n 182 41% 47 29% 59 30% 76\\n 68 26% 18 32% 20 42% 30\\n 26% 30% 44%\\n The Pisces supercluster contains 10 X-ray clusters, 4 of which are non-Abell clusters. However, our analysis shows that actually these poor clusters belong to a filament that connects the Pisces supercluster and superclusters 211 and 215.\\n Poor X-ray clusters connect the Coma and the Leo super- clusters (SCL 117 and 93), the Sculptor supercluster (SCL 9) and SCL 220 (see also Paper II), SCL 126 and 136, and SCL 212 and 297. These cases confirm that poor X-ray clusters trace the supercluster-void network determined by Abell clusters. X-ray clusters either belong to superclusters themselves or they form filaments between them.\\n Additional superclusters of X-ray clusters from Table B2, being located in filaments between superclusters, also trace the supercluster-void network. Several of these systems (SCLX 7, 9, and 12) border the Southern and Northern Local Supervoids (EETDA). SCLX 9 contains one of the X-ray brightest Abell clusters, A496, see above, and in ad- dition to poor clusters this system harbors two X-ray de- tected AGNs, RBS 550 and RBS 556. X-ray detected AGNs from the RBS catalog connect SCLX 4 and 7 from Ta- ble B2. This joint system contains 11 AGNs and 7 X-ray selected clusters, including 3 Abell clusters and one QSO (QSO \\n In EETDA we showed that isolated Abell clusters are lo- cated close to the superclusters and do not fill in the voids between superclusters. Our present analysis shows addi- tionally that most of the isolated poor X-ray clusters that 24 h−1 Mpc are located in fil- do not have neighbors at R aments between superclusters or on the borders of Southern and Northern Local voids.\\n In Figure 2 we plot the distribution of X-ray clusters and Abell clusters that belong to very rich superclusters.\\nWe see that the structures delineated by optical and X-ray clusters coincide and we can see a pattern of superclusters and voids. The supercluster-void network is more clearly seen in three-dimensional animations from our web page, www.aai.ee.\\n In this Figure we plot also clusters from additional su- perclusters (Table B2), as well as the location of isolated non-Abell clusters. Many of them are located near the zone of avoidance where cluster catalogs tend to be incomplete and superclusters cannot be determined.\\n 3.3. Fraction of X-ray clusters in superclusters\\n After compiling the list of X-ray clusters in superclusters we calculate the fractions of these clusters in superclusters of various richness (Table 1). Superclusters are divided into richness classes as in E97c: poor superclusters (number of members NCL = 2, 3), rich superclusters (4 7), and very rich superclusters (NCL 8). Additionally, we give the fraction of isolated X-ray clusters.\\n NCL\\n Table 1 shows that the fraction of X-ray clusters in su- perclusters increases with increasing supercluster richness.\\nThe Kolmogorov-Smirnov test confirms that the zero hy- pothesis (the distributions of optical and X-ray clusters in superclusters of various richness are statistically identical) is rejected at the 99% confidence level. In total, about one third of all superclusters and 23 of 29 very rich superclus- ters contain X-ray clusters. About 25% of Abell clusters are isolated at the neighborhood radius R = 24 h−1 Mpc.\\nIn contrast, only about 15% of X-ray clusters are isolated at this radius.\\n We note that various surveys used in the present study show a similar tendency – the increase of the fraction of X- ray clusters with supercluster richness. However, the exact percentages of X-ray clusters in systems of various rich- ness are somewhat different due to the differences between samples. For example, due to the sky coverage limits the fraction of isolated clusters is relatively high in the BCS sample (25% of poor clusters in this sample are isolated, see also Paper II). Also, due to the incompleteness of X-ray cluster catalogs at large distances these fractions should actually be taken as lower limits: at distances larger than R = 275 h−1 Mpc there are only five supercluster with more than one X-ray member cluster, and over 20 superclusters containing one X-ray cluster only. However, test calcula- tions with smaller, statistically more complete subsample from RBSC catalog in which clusters were selected up to the distance R = 250 h−1 Mpc (Paper II) confirm that the fraction of X-ray clusters in rich superclusters is higher than in poor superclusters.\\n 3.4. X-ray luminosities of clusters in superclusters of\\n different richness\\n In Figure 3 we plot X-ray luminosities for clusters in su- perclusters of different richness in units of \\nX-ray luminosities are calculated differently in the various In some catalogs the broad en- X-ray cluster catalogs.\\nergy band (0.1 -2.4 keV) is used (e.g. the BCS sample), while others are based on the hard energy band (0.5 - 2.0\\n Fig. 3.— X-ray luminosities for clusters in superclusters of different richness and for isolated clusters (in units of \\n keV). Also, different methods are used to determine the total X-ray flux of extended sources. As a result, the X- ray luminosities for various cluster samples are not directly comparable, particularly in the case of clusters with com- plicated morphology. However, our aim is to see whether cluster X-ray luminosities are correlated with host super- cluster richness, and for that purpose we may simply plot X-ray luminosities for each sample separately.\\n Figure 3 shows that some clusters of very high X-ray luminosity are located in superclusters of low multiplicity.\\nSince Figure 3 does not show any other clear correlation be- tween cluster X-ray luminosities and their host supercluster richness we think that it is preliminary to draw quantitative conclusions from this finding. Instead, we describe shortly the locations and properties of the brightest X-ray clusters.\\nThe cluster with the highest X-ray luminosity in the\\n Northern sky is A\\n The second brightest X-ray cluster in the Northern sky, A\\n The third brightest X-ray cluster in the RBSC catalog is A401 which forms a cluster pair with A399 (SCL 45). Both\\n of these clusters contain a cD galaxy. MFSV suggest that these clusters may be in the early stages of a collision.\\n Another isolated cluster of high X-ray luminosity, A478, shows evidence for a strong cooling flow (MFSV and White, Jones and Forman \\nIn clusters A478 and A\\n One of the clusters of the highest X-ray luminosity in the DFJ sample is A426, a cooling flow cluster (White, Jones and Forman \\n The brightest X-ray cluster in the sample by de Grandi et al. (\\n The second brightest X-ray cluster in the sample by de Grandi et al. (\\n The\\n in de Grandi’s\\n third brightest cluster\\n sam- ple is A\\nthis clus- ter is probably dominated by its central galaxy that shows signs of merging of other galaxies in the clus- ter (Astronomy Picture of the Day, August 31, \\n X-ray emission of\\n 4. DISCUSSION AND CONCLUSIONS\\n We have studied the distribution of X-ray clusters with respect to the supercluster-void network determined by Abell clusters, compiled a list of X-ray clusters in super- clusters and showed that both X-ray and optical clusters delineate large-scale structure in a similar way. X-ray clus- ters that do not belong to superclusters determined by Abell clusters border the Southern and Northern Local supervoid or are located in filaments between superclusters. X-ray clusters are more strongly clustered than optically selected clusters: the fraction of X-ray clusters is higher in rich and very rich superclusters, and the fraction of isolated X-ray clusters is lower than these fractions for optically selected clusters. These results indicate that the structure of the Universe is traced in a similar way by both optical and X- ray clusters up to redshifts of z = 0.13. A similar conclusion has been obtained by Borgani & Guzzo (\\nThe rather regular placement of superclusters is notice- able in the case of both X-ray clusters and Abell clusters, es- pecially in the Northern sky. We shall discuss the presence of the regularity in the distribution of X-ray clusters in more detail in Paper II. In particular, we shall present evidence for a presence of a characteristic scale of 120h−1 Mpc in the distribution of X-ray clusters.\\n EETDA demonstrated that the fraction of X-ray clus- ters in superclusters increases with supercluster richness (Table 4 in EETDA). This result was based on the early catalogs of X-ray clusters containing altogether 59 X-ray\\n clusters in superclusters. Our present study confirms and even strengthens this early result. The data in Table 1 show that the fraction of X-ray clusters in the Abell cluster-based superclusters increases with supercluster richness. In sev- eral superclusters most members are X-ray sources. The presence of X-ray emitting gas in a large fraction of clus- ters shows that potential wells in clusters and superclusters of galaxies are rather deep.\\n We did not detect a correlation between the X-ray lu- minosity of clusters and their host supercluster richness, although clusters with the highest X-ray luminosities are located in relatively poor superclusters.\\n Loken et al. (\\n Engels et al. (\\n Boughn (\\n These findings give additional evidence that superclus- ters are not random associations of clusters but form real physical systems – large-scale high-density regions of the matter distribution forming extended potential wells in the distribution of matter. Both optical and X-ray clusters are parts of the same supercluster-void network that we see in the distribution of Abell clusters of galaxies. Our results suggest that optically and X-ray selected cluster samples can be used to find large-scale high-density regions in the Universe. Samples detected optically and in X-rays are dif- ferent in many details, but are common in one important aspect – both indicate the skeleton of the supercluster-void network in a rather similar way.\\n Main results of our study of the clustering properties of\\n X-ray clusters are:\\n 1) We present an updated catalog of superclusters of Abell clusters and a list of X-ray clusters in superclusters.\\n2) Optical and X-ray clusters trace the supercluster-void\\n network in a similar way.\\n 3) The fraction of X-ray clusters in superclusters increases with the supercluster richness suggesting that superclusters are real physical systems.\\n 4) Cluster X-ray luminosity is not correlated with their host supercluster richness, although the most luminous X- ray clusters are located in relatively low density environ- ments.\\n APPENDIX A: A CATALOG OF SUPERCLUSTERS OF ABELL CLUSTERS\\n Here we present a new supercluster catalog based on the\\n Abell cluster sample (A1) used in this paper.\\n The catalog of superclusters of Abell clusters is based on a cluster sample which contains all superclusters of richness class NCL 2. Table A1 contains the following entries: N o is the identification number. The supercluster should be re- ferred to as “SCL nnn” with nnn being the running number N o. As mentioned in the text, an index ”c” in the first col- umn indicates a supercluster candidate, i.e. a supercluster that is not present in the test catalog determined by clusters of measured redshifts only.\\n NCL is the number of member clusters in the superclus- ter; αC and δC are coordinates of the center of the super- cluster (equinox \\n APPENDIX B: X-RAY CLUSTERS IN SUPERCLUSTERS\\n In Table B1 we present data on X-ray clusters in super- clusters, while Table B2 lists additional systems of X-ray\\n clusters. Columns for both tables are as follows:\\n (1) identification number of the supercluster in the cata log; subscript C means supercluster candidate;\\n (2) Abell numbers of all clusters in the supercluster, ac cording to Table A1;\\n (3) , (4) and (5) – center coordinates for the supercluster\\n (α, δ and distance to the supercluster center);\\n (6): Catalog numbers of X-ray clusters in the superclus- ter. We use Abell - ACO catalog numbers for clusters iden- tified in this catalog. Cluster numbers without subscript are from RBSC catalog; index G means clusters from de Grandi et al. (\\n Double subscripts refer to non-Abell clusters. Index RR means clusters number from RBS catalog; index BB – clus- ter number from BCS catalog; index GG – cluster number from the catalog by de Grandi et al. (\\n In Table B2 clusters without subscripts refer to Abell clusters that are not listed in the X-ray cluster catalogs used in the present study.\\n (7): identification of supercluster.\\n We thank G¨unther Hasinger for providing us with a draft version of the RBS catalog and discussion of preliminary re- sults of the study. We thank Enn Saar and Alexei Starobin- sky for stimulating discussion. This work was supported by Estonian Science Foundation grant',\n",
       " 'metadata': {'topic': 'Machine Learning',\n",
       "  'keyword': 'clustering',\n",
       "  'filename': '0012536v1.pdf'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens_llama(all_documents[-1]['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': '2005.07474v1',\n",
       " 'documents': 'Robot Accident Investigation: a case study in Responsible Robotics\\n Alan F.T. Winfield, Katie Winkle, Helena Webb, Ulrik Lyngs, Marina Jirotka and Carl Macrae\\n Abstract Robot accidents are inevitable. Although rare, they have been happening since assembly-line robots were first introduced in the \\n A Winfield, K Winkle Bristol Robotics Lab, UWE Bristol, UK. e-mail: alan.winfield@brl.ac.uk,katie.\\nwinkle@brl.ac.uk\\n H Webb, U Lyngs, M Jirotka Department of Computer Science, University of Oxford, UK. e-mail: helena.webb@cs.ox.\\nac.uk,ulrik.lyngs@cs.ox.ac.uk,marina.jirotka@cs.ox.ac.uk\\n C Macrae Nottingham University Business School, University of Nottingham, UK. e-mail: Carl.Macrae@ nottingham.ac.uk\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n 1 Introduction\\n What could possibly go wrong?\\nImagine that your elderly mother, or grandmother, has an assisted living robot to help her live independently at home. The robot is capable of fetching her drinks, reminding her to take her medicine and keeping in touch with family. Then one afternoon you get a call from a neighbour who has called round and sees your grandmother collapsed on the floor.\\nWhen the paramedics arrive they find the robot wandering around ap- parently aimlessly. One of its functions is to call for help if your grand- mother falls or stops moving, but it seems that the robot failed to do this.\\n Fortunately your grandmother makes a full recovery. Not surprisingly you want to know what happened: did the robot cause the accident? Or maybe it didn’t but made matters worse, and why did it fail to raise the alarm?\\n Although this is a fictional scenario it could happen today. If it did we would be to- tally reliant on the goodwill of the robot manufacturer to discover what went wrong.\\nEven then we might not get the answers we seek; it is entirely possible that neither the robot nor the company that made it are equipped with the tools and processes to undertake an investigation.\\n Robot accidents are inevitable. Although rare, they have been happening since assembly line robots were first introduced in the \\n In contrast second wave robots are designed to operate in human environments and interact directly with people. Those human environments include homes, of- fices, hospitals, shopping malls, and city or urban streets and – unlike first wave robots – many are designed to be used by untrained, naive or vulnerable users, including children and the elderly. These are robots in society, and hence social robots1. Often with sophisticated embedded artificial intelligence (AI) social robots might be deployed as care robots to assist elderly or disabled people to live indepen- dently. Smart robot toys offer a compelling interactive play experience for children and increasingly capable autonomous vehicles (AVs) the promise of hands-free per- sonal transport and fully autonomous taxis.\\n 1 Noting that we take a broader view of social robotics than usual.\\n Robot Accident Investigation\\n Social robots by definition work with and alongside people in human environ- ments, thus the likelihood and scope of robot accidents is much greater than with industrial robots. This is not just because of the close proximity of social robots and their users (and perhaps also bystanders), it is also because of the kinds of roles such robots are designed to fulfill, and further exacerbated by the unpredictability of people and the unstructured, dynamic nature of human environments.\\n Given the inevitability of social robot accidents it is perhaps surprising that no frameworks or processes of social robot accident investigation exist. This paper ad- dresses that deficit by setting out a draft framework for social robot accident inves- tigation; a framework which proposes both the technology and processes that would allow social robot accidents to be investigated with no less rigour than we expect of air or rail accident investigations.\\n This paper proceeds as follows. We first survey the current practices and frame- works for accident investigation, including in transport (air, rail and road) and in healthcare, in section 2. In section 3 we survey robot accidents, including both in- dustrial and social robot accidents, then analyse the scope for social robot accidents in order to understand why social robot accidents are more likely (per robot de- ployed) than industrial robot accidents. Section 4 then places accident investigation within the practice of responsible robotics, by defining responsible robotics within the broader practice of Responsible Innovation; the section also briefly surveys the emerging practices of values-driven design and ethical standards in robotics. Section 5 sets out both the technology and processes of our draft framework for social robot accident investigation, then illustrates the application of this framework by setting out how an investigation of our fictional accident might play out. Finally, in section 6, we conclude by outlining work currently underway within project RoboTIPS to develop and validate our framework with real-robot mock accident scenarios, be- fore considering some key questions about who would investigate real-world social robot accidents.\\n 2 The practice of Accident Investigation\\n Investigating accidents and incidents is a routine and widespread activity in safety- critical industries such as aviation, the railways and healthcare. In healthcare, for instance, over 2 million safety incidents are reported across the English National Health Service each year, with around 50,000 of those incidents causing moderate to severe harm or death [26] – such as medication errors or wrong-site surgery. Ac- cident investigation also has a long history. In aviation, the world’s first national air accident investigation agency was established over a century ago in \\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n tion of social robot accidents, it is instructive to examine how accident investigation is conducted in other safety-critical domains.\\n First, it is important to emphasise that the core principle and fundamental pur- pose of accident investigation is learning: while investigations primarily focus on examining events that have occurred in the past, the core purpose of an accident investigation is to improve safety in the future. Accident investigations are therefore organised around three key questions [23]. The first is factual: what actually hap- pened? The second is explanatory: why did those things happen and what does that reveal about weaknesses in the design and operation of a particular system? The third is practical: how can systems be improved to prevent similar events happening in future? The ultimate objective for any accident investigation is to develop practi- cal, achievable and targeted recommendations for making safety improvements.\\n Conducting an accident investigation involves collecting and analysing a range of evidence and data from a variety of sources to understand what happened and why.\\nThis can include quantitative data, such as information from ‘black box’ Flight Data Recorders on aircraft that automatically collect data on thousands of flight parame- ters. It can also include qualitative data in the form of organisational documentation and policies, and in-depth interviews with witnesses or those directly or indirectly involved in an accident such as a pilot or a maintenance engineer. Accident in- vestigators therefore need to be experts in the methods and processes of accident investigation, and also need to have a deep and broad understanding of the industry and the technologies involved. However, accident investigations are also typically collaborative processes that are conducted by a diverse team of investigators who, in turn, draw on specific sources of expertise where required [20].\\n A variety of methods have been developed to assist in the collection and analysis of safety data, from cognitive interviewing techniques [13] to detailed human fac- tors methods [36] and organisational and sociotechnical systems analysis techniques [37]. Importantly, to understand why an accident has occurred and to determine how safety can be improved in future, accident investigations typically apply a systemic model of safety that helps identify, organise and explain the factors involved and how they are interconnected. One of the most widely applied and practical acci- dent models is the organisational accident framework – commonly referred to as the Swiss Cheese model [33]. This has been adapted and applied in various ways [2], but at core this provides a simple framework that conceptualises system safety as dependent on multiple layers of risk controls and safety defences that span from the front-line to organisational and regulatory levels – such as redundant systems, emergency alarms, operator training, management decisions or regulatory require- ments. Each of these safety defences will inevitably be partial or weak in some way, and accidents occur when the holes in these defences all line up in some unexpected way – thus the eponymous image of multiple slices of Swiss Cheese, each slice full of holes. The focus of accident investigations is to examine the entire sociotechnical system to understand the safety defences, the weaknesses in those defences, why those weaknesses arose and how they can be addressed. Similar premises underpin a variety of different accident investigation models, methods and tools.\\n Robot Accident Investigation\\n Safety-critical industries have developed sophisticated systems to support these activities of investigation and learning at all levels. These range from lengthy in- vestigations of major accidents that are coordinated by national investigative bod- ies to relatively rapid local-level investigations of more minor incidents or near- miss events that are conducted by investigators within individual organisations [21].\\nA lot of media attention understandably focuses on the investigations into high- profile accidents that are conducted by national investigation bodies, such as the US National Transportation Safety Board’s investigations of the various accidents in- volving semi-automated Tesla vehicles [4] and Uber’s autonomous test vehicle [5].\\nHowever, much of the more routine work of investigation actually occurs within individual organisations, like airlines and hospitals, which regularly conduct hun- dreds or thousands of investigations each year. These local-level investigations ex- amine more minor safety incidents as well as near-miss events – where there was no adverse outcome but some sort of safety-relevant failure was detected, such as a poorly specified maintenance procedure in an airline leading to a technical failure that causes a rejected take-off [21]. Local-level investigations employ similar meth- ods and approaches to those conducted at a national level, but are often much more rapid, lasting days or weeks rather than months and years. They are also typically limited to examining sources of risk within one single organisation, unlike national- level investigations which can consider regulatory factors and interactions between various different organisations across an industry.\\n At all these levels of investigative activity, accident and incident investigation is always focused on learning. One of the main implications of this focus on learning is that accident investigation activities are typically entirely separated from other investigative activities that seek to attribute blame or determine liability. In aviation, for example, the information collected during major accident investigations may only be used for the purposes of safety investigation and improvement – and may not be used, for instance, to punish individuals or pursue legal claims against organ- isations (EU \\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n 3 Robot Accidents\\n Robert Williams is believed to be the first person killed by a robot in an industrial accident, in January \\n Finding data on accidents involving robots which interact with humans (HRI) is also difficult. One study on the safety of interactive industrial robots in Finland [24] notes that “International accident-report data related to robots is scarce”. The study reports that the majority of the \\n • “The cause of an accident is usually a sequence of events, which have been diffi cult to foresee.\\n • Operator inattentiveness and forgetfulness may lead them to misjudge a situation\\n and even a normal robot movement may surprise them.\\n • Most of the accidents involving robots occurred so that (the) robot moved un- expectedly (from worker’s point of view) against the worker within the robot working area.\\n • Inadequate safeguarding featured significantly as a cause of accidents.\\n• Many accidents are associated with troubleshooting disturbances. And • only about 20% of accidents occurred during undisturbed automated runs.” [24]\\n Although now somewhat dated, Chapter 4 ‘Robot Accidents of [11] sets out a comprehensive analysis of industrial robot accidents, including data from accidents in Japan, Western Europe and the United States. Noting that “human behavior plays an important role in certain robot accidents” the paper outlines a set of typical human\\n 2 https://en.wikipedia.org/wiki/Robert_Williams_(robot_fatality) 3 https://www.osha.gov/pls/imis/AccidentSearch.search?acc_keyword= %22Robot%22&keyword_list=on 4 https://www.cdc.gov/niosh/topics/robotics/aboutthecenter.html\\n Robot Accident Investigation\\n behaviours that might result in injury. A number of these are especially instructive to the present study:\\n • “Humans often incorrectly read, or fail to see, instructions and labels on various\\n products.\\n • Many people carry out most assigned tasks while thinking about other things.\\n• In most cases humans use their hands to test or examine.\\n• Many humans are unable to estimate speed, distance, or clearances very well. In fact, humans underestimate large distances and overestimate short distances.\\n • Many humans fail to take the time necessary to observe safety precautions.\\n• A sizeable portion of humans become complacent after a long successful expo sure to dangerous items.\\n • There is only a small percentage of humans which recognize the fact that they cannot see well enough, either due to poor illumination or poor eyesight.” [11]\\n Finding data on non-industrial robot accidents is even more difficult, but one study reports on adverse events in robotics surgery in the US [1]; the paper sur- veys 10,624 reports collected by the Food and Drug Administration between \\n A recent study in human-robot interaction examines several serious accidents, in aviation, the nuclear industry, and autonomous vehicles in an effort to understand “the potential mismatches that can occur between control and responsibility in au- tomated systems” and argues that these mismatches “create moral crumple zones, in which human operators take on the blame for errors or accidents not entirely in their control” [12].\\n 3.1 The Scope for Social Robot Accidents\\n As we have outlined above, industrial and surgical robot accidents are – thankfully – rare events. It is most unlikely that social robot accidents will be so rare. There are several factors that lead us to make this forecast:\\n 1. Social robots, as broadly defined in this paper, are designed to be integrated into society at large: in urban and city environments, schools, hospitals, shopping malls, and in both care and private homes. Unlike industrial robots operating\\n https://en.wikipedia.org/wiki/List_of_self-driving_car_\\n fatalities 6 https://www.wsj.com/articles/security-robot-suspended-after-colliding-with-a-toddler-\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n behind safety cages social robots are designed to be among us, up close and personal. It is this close proximity that undoubtedly presents the greatest risk.\\n2. Operators of industrial robots are required to undertake training courses, both on how to operate their robots and on the robot’s safety features, and are expected to follow strict safety protocols. In contrast social robots are designed to benefit naive users – including children, the elderly, and vulnerable or disabled people – with little more preparation than might be expected to operate a vacuum cleaner or dishwasher.\\n 3. Industrial robots typically operate in an environment optimised for them and not humans. Social robots have no such advantage: humans (especially young or vulnerable humans) are unpredictable and human environments (from a robots perspective) are messy, unstructured and constantly changing. Designing robots capable of operating safely in human environments remains a serious challenge.\\n4. The range of serious harms that can arise from social robots is much greater than those associated with industrial robots. Social robots can, like their indus- trial counterparts, cause physical injury, but the responsible roboticist should be equally concerned about the potential for psychological harms, such as decep- tion (i.e. a user coming to believe that a robot cares for them), over-trust or over- reliance on the robot, or violations of privacy or dignity. A number of ethical hazards are outlined in section 4 below7.\\n 5. The scope of social robot accidents is thus enlarged. The nature of the roles social robots play will play in our lives – supporting elderly people to live independently or helping the development of children with autism for instance [8] – opens up a range of ethical risks and vulnerabilities that have hitherto not been a concern of either robot designers or accident investigators. This factor also increases the likelihood of social robot accidents.\\n If we are right and the near future brings an increasing number of social robot accidents, these accidents will need to be investigated in order to address the three key questions of accident investigation outlined in section 2. What happened? Why did it happen? And what must we change to ensure it doesnt happen again? [23].\\n 4 Responsible Robotics\\n In essence, Responsible Robotics is the application of Responsible Innovation (RI) to the field of robotics, so we first need to define RI. A straightforward definition of RI is: a set of good practices for ensuring that research and innovation bene- fits society and the environment. There are several frameworks for RI. One is the EPSRC AREA framework8, built on the four pillars of Anticipation (of potential impacts and risks), Reflection (on the purposes of, motivations for and potential implications of the research), Engagement (to open up dialogue with stakeholders\\n 7 An ethical hazard is a possible source of ethical harm 8 https://epsrc.ukri.org/research/framework/area/\\n Robot Accident Investigation\\n beyond the narrow research community), and Action (to use the three processes of anticipation, reflection and engagement, to influence the direction and trajectory of the research) [30]. The more broadly framed Rome Declaration on Responsible Re- search and Innovation9 is built around the six pillars of open access, governance, ethics, science communication, public engagement and gender equality10.\\n We define Responsible Robotics as follows:\\n Responsible Robotics is the application of Responsible Innovation in the design, manufacture, operation, repair and end-of-life recycling of robots, that seeks the most benefit to society and the least harm to the environment.\\n Robot ethics – which is concerned with the ethical impact of robots, on individ- uals, society and the environment, and how any negative impacts can be mitigated – and ethical governance both have an important role in the practice of responsi- ble robotics. In recent years many sets of ethical principles have been proposed in robotics and AI – for a comprehensive survey see [17] – but one of the earliest are the EPSRC Principles of Robotics, first published online in \\n Responsible social robotics [38] is the practice of responsible robots in society with a particular ambition of creating robots which bring real benefits in both quality of life and safeguarding to the most vulnerable, within a framework of values-based design based upon a deep consideration of the ethical risks of social robots. There are a number of approaches and methods available to the responsible roboticist, including emerging new ethical standards, and an approach called ethically aligned design, which we will now review.\\n 4.1 Ethically Aligned Design\\n In April \\n https://ec.europa.eu/research/swafs/pdf/rome_declaration_RRI_\\n final_21_November.pdf 10 http://ec.europa.eu/research/science-society/document_library/ pdf_06/responsible-research-and-innovation-leaflet_en.pdf 11\\n https://epsrc.ukri.org/research/ourportfolio/themes/\\n engineering/activities/principlesofrobotics/ 12\\n https://standards.ieee.org/content/dam/ieee-standards/\\n standards/web/documents/other/ec_about_us.pdf\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n well-being as its central tenet. The initiative’s mission is “to ensure every stake- holder involved in the design and development of autonomous and intelligent sys- tems (AIS) is educated, trained, and empowered to prioritize ethical considerations so that these technologies are advanced for the benefit of humanity”.\\n The first major output from the IEEE global ethics initiative is a document called Ethically Aligned Design (EAD) [16]. Developed through an iterative process over 3 years EAD is built on the three pillars of Universal Human Values, Political self-determination & Data Agency, and Dependability, and eight general (ethical) principles covering Human Rights, Well-being, Data Agency, Effectiveness, Trans- parency, Accountability, Awareness of Misuse and Competence, and sets out more than 150 issues and recommendations across its 10 chapters. In essence EAD is both a manifesto and roadmap for a values-driven approach to the design of autonomous and intelligent systems. Spiekermann and Winkler [35] detail the process of ethi- cally aligned design within a broader methodological framework they call value- based engineering for ethics by design. It is clear that responsible social robotics must be values-based.\\n 4.2 Standards in Social Robotics\\n A foundational standard in social robotics is ISO \\n A new generation of explicitly ethical standards are now emerging [39, 27].\\nStandards are simply “consensus-based agreed-upon ways of doing things” [9]. Al- though all standards embody a principle or value, explicitly ethical standards ad- dress clearly articulated ethical concerns and – through their application – seek to remove, reduce or highlight the potential for unethical impacts or their consequences [39].\\n The IEEE ethics initiative outlined above has initiated a total of 13 working groups to date, each tasked with drafting a new standard within the \\n Almost certainly the worlds first explicitly ethical standard in robotics is BS\\n“BS\\n Robot Accident Investigation\\n\\n ical risks so identified. At its heart is a set of 20 distinct ethical hazards and risks, grouped under four categories: societal, application, commercial and financial, and environmental. Advice on measures to mitigate the impact of each risk is given, along with suggestions on how such measures might be verified or validated”[39].\\nSocietal hazards include, for example, anthropomorphisation, loss of trust, decep- tion, infringements of privacy and confidentiality, addiction, and loss of employ- ment, to which we should add the Uncanny Valley[25], weak security, lack of trans- parency (for instance the lack of data logs needed to investigate accidents), unre- pairability and unrecyclability. Ethical risk assessment is a powerful and essential addition to the responsible roboticists toolkit, as it can be thought of as the opposite face of accident investigation, seeking – at design time – to prevent risks becoming accidents.\\n 5 A Draft Framework for Social Robot Accident Investigation\\n We now set out a framework for social robot accident investigation outlining first the technology, then the process, followed by an illustration of how the framework might be applied.\\n 5.1 Technology\\n We have previously proposed that all robots should be equipped with the equivalent of an aircraft Flight Data Recorder (FDR) to continuously record sensor inputs, actuator outputs and relevant internal status data [40]. We call this an ethical black box15 (EBB), and argue that the EBB will play an important role in the process of discovering how and why a robot caused an accident.\\n All robots collect sense data, then – on the basis of that sense data and some in- ternal decision making process (the embedded Artificial Intelligence) – send com- mands to actuators. This is of course a simplification of what in practice will be a complex set of connected systems and processes but, at an abstract level, all intelli- gent robots will have the three major subsystems shown in blue, in Fig. 1 Our EBB will have much in common with its aviation and automotive counterparts, the flight data recorder [15] and event data recorder (EDR) [14], in particular: data is stored securely in a rugged unit designed to survive accidents; stored records are time and date stamped, and storage capacity is limited and organised such that only the most recent data are saved – overwriting older records (an FDR typically records 17-25 hours of data, an automobile EDR as little as 30 seconds).\\n The technical specification for an EBB for a social robot is beyond the scope of this paper. It is, however, important to outline here the kinds of data we should\\n 15 Because it would be unethical not to have one.\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n Fig. 1 Robot sub-systems with an Ethical Black Box and key dataflows\\n expect to be recorded in the EBB. Consider the Pepper robot, as an exemplar of an advanced social robot [31]. The Pepper is equipped with 25 sensors, including four microphones, two colour cameras, two accelerometers and various proximal and distal sensors. It has 20 motors, a chest mounted touch display pad, loudspeakers for speech output, and WiFi connectivity. We would therefore expect an EBB designed for the Pepper robot to be able to record:\\n 1. Records of input sense data, including sampled (and compressed) camera images, audio sampled by the microphones, accelerometers, and touch screen touches; 2. Actuator demands generated by the robot’s control system along with sampled\\n position of joints, from which we can deduce the robots pose;\\n 3. Synthesised speech and touch screen display outputs; 4. Periodically sampled battery levels; 5. Periodically sampled status of the robot’s WiFi and Internet connectivity, and 6. The robot’s position (i.e. x,y coordinates) within its working environment (noting that this data might be available from a tracking system in a ‘smart’ environment, or if not, deduced via odometry from the robot’s known start position at, say, its re-charging station and subsequent movements).\\n The EBB should also record high level decisions generated by the robots AI (see the data flow in Fig. 1), and – given that social robots are likely to accept speech\\n Robot Accident Investigation\\n\\n commands – we would, ideally, be able to record both the raw audio recorded by the microphones and the text sequence produced by the robot’s speech recogniser.\\n 5.2 Process\\n Conventionally accident investigation is performed in four stages: (1) information gathering, (2) fact analysis, (3) conclusion drawing and – if necessary – (4) im- plementation of remedies. Stage (2) fact analysis might typically take the form of causal analysis, and we propose to adopt here the method of why-because analysis developed by Ladkin et al. [18, 19, 34].\\n Why-Because Analysis (WBA) is a method of causal analysis of complex socio- technical systems, and hence well suited to social robot accidents. WBA lends itself to a simple quality check: whether one event is a causal factor in another can be determined by applying a counter-factual test. The primary output of WBA is a Why-Because Graph (WBG), and a further advantage of WBA is that – if necessary – the correctness of the WBG can be formally proven. Fig. 2 shows, in flow chart form, the process of why-because analysis.\\n Let us elaborate briefly on some of the steps in Fig. 2. ‘Gather information’ re- quires collecting all available information on the accident. This will include witness testimony, records from the EBB, any forensic evidence, and contextual information on the setting of the accident. The next stage ‘determine facts’ requires the investi- gation team to sift the information and glean the essential facts. Noting that witness testimony can be unreliable, any causal inferences from that testimony should ide- ally be corroborated with forensic evidence, so that – even if not absolutely certain – the team can have high confidence in those inferences. The third stage: ‘create why-because list’ links the facts of events – including both things that happened and things that should have happened but did not (unevents) – to outcomes. If they give the team a clear picture of the sequence of events and participants in the acci- dent then the team agree on the ‘mishap topnode(s)’ of the why-before graph, i.e.\\nthe accident – or perhaps multiple accidents. Then the why-because graph is cre- ated, top-down. This is likely also to required several iterations and – if necessary – quality checking using counter-factual tests or formal proof of completeness. For a complete explanation of the method refer to the the introduction to WBA in [34].\\n 5.3 The application of the framework\\n To understand how this framework would operate in practice, we return to the fic- tional scenario outlined at the start of the paper. As described in the scenario, an elderly lady Rose has a fall in her home. She is found, still on the floor, some time later by her neighbour, Barbara. Barbara calls for medical help as well as alert- ing Rose’s family. Whilst Rose receives hospital treatment, an investigation team is\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n Fig. 2 An overview of Why-Because Analysis (adapted from Ladkin, \\n formed, who begin to collect evidence for the investigation. Photos of Rose’s flat are taken and information about her home set up is collected; for instance, Rose lives in a smart home with various sensors to detect her movements and communicate with the robot as necessary. Preliminary observation of the robot also reveals details about its design and use in the home. The robot can fetch drinks, provide reminders (such as for Rose to take medication) and act as an interface so that Rose can con- trol her television and other devices through it with voice commands. The robot will also respond to commands sent via a smartphone/tablet application.\\n Robot Accident Investigation\\n\\n Barbara, the paramedics and Rose herself are interviewed to provide witness statements. These statements combine with the initial observations to provide im- portant early findings that shape the ongoing investigation. Of key importance are the following: i) Rose didn’t put on her fall alert bracelet on the day of the accident, and ii) From her condition (as observed by the paramedics) it seems that after her fall Rose was too weak to call out to the robot if it was more than two metres away from her.\\n In her witness testimony Rose states that she had climbed on a chair and was reaching for something in an upper cupboard in her kitchen but then slipped and fell on the floor. She has no memory of what happened after that and does not recall where the robot was when she fell. Barbara states that she doesn’t recall seeing the robot when she entered the flat, and feels that the shock of finding Rose on the floor meant she didn’t notice much else around her. The paramedic states that he noticed the robot moving about around the flat – between the living area and the kitchen.\\nIt didn’t seem to be moving for the accomplishment of any particular action so he described the robot as acting aimlessly.\\n The investigation team gather further information. They interview the manager of the retirement complex that Rose lives in; she provides details of the organisational structure of the complex including the infrastructure that enables residents to set up their homes as smart homes and have assistance robots. They also talk to others who saw Rose on the day of her accident. The last person to see Rose before her fall was Michelle, a cleaner who works for the retirement complex. Michelle saw Rose, whilst she was in Rose’s flat for an hour to do her regular weekly clean. Michelle reported that Rose seemed very cheerful and chatty, and did not complain of feeling ill or mention any worries or concerns. Michelle said that she did her usual clean in its usual order as requested by the retirement complex: collect up rubbish to take outside; wipe bathroom surfaces and floor; wipe kitchen work surfaces and clean floor; polish wooden surfaces; hoover carpeted areas; disinfectant wipes on door handles and all over the robot for infection control. When asked by the investigation team she said she thought the robot was in the living room for most of the time she was there but she couldn’t really remember. She didn’t notice anything unusual about what the robot was doing.\\n The team also gets a report from Rose’s General Practitioner about her overall health status prior to the accident. This states that Rose had some limited mobil- ity and needed to take daily medication to help her arthritis. She had also recently complained of forgetting things more and more easily. However she was generally healthy for her age and had no acute concerns.\\n Finally, the team extracts data from the Ethical Black Box. These are in CSV format, and contain timestamped information regarding (i) the location and status of the robot and Rose/others within the apartment, (ii) actions undertaken by the robot and (iii) sampled records of all other robot inputs/outputs over the previous 24 hour period. It enables the team to conclude that the robot lost connection to the central ‘home connection hub’ intermittently over the past 24 hours, coinciding with Rose’s fall. In addition, processing of the camera feed and other sensors used for navigation appear to be producing erroneous data. The records showed no log\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n of Rose’s fall, but did log that the robot made a number of ‘requests for help’ – by speaking out loud – regarding its inability to connect to the home connection hub.\\n Having collected and analysed all of the material, the investigation team identify key relevant factors. At the individual level, certain actions by Rose – forgetting to put on her accident bracelet and reaching to a high cupboard – certainly increased the safety risk. Aspects of the local environment are likely to have also contributed to this risk and influenced the technical problems that occurred – for instance the repeated disinfecting of the robot, as required by the retirement complex, has almost certainly impaired its sensors. The robot’s response to losing connection to the home hub, i.e. asking for help was clearly not effective in getting the problem addressed, most likely because Rose did not understand the problem.\\n Concerning the robots standard functionalities, it failed to detect Rose’s fall and therefore raise an alert following the fall. The robot’s fall detection system relies, in part, on data collected by distributed sensors placed around the smart home. This data is delivered to the robot via the home connection hub, so the intermittent con- nectivity issues prevented the robot’s fall detection functionality from operating as intended. The team make use of these key facts to construct the why-because graph shown in Fig. 3.\\n The first thing to note in Fig. 3 is that there are two mishap topnodes and hence two sub-graphs. On the left is the sub-graph showing the direct causal chain that resulted in Rose’s fall (Accident 1), following her unwise decision to try and reach a high cupboard. The sub-graph on the right shows the chain of factors – processes, states and unevents (things that should have happened but didn’t) – that together led to Accident 2: the robot failing to raise the alarm.\\n The sub-graph leading to Accident 2 shows that the four ways in which the robot might have raised the alarm all failed, for different reasons. The first (a) is that the robot was too far away from Rose to hear her calls for help (most likely because the failure of its connection with the home hub means that the robot didn’t know where she was). The second (b) is that the robot’s sensors that should have been able to detect her fall (together with data from the smart environment) were damaged, almost certainly by cleaning with disinfectant, and the third (d) was the failure of the wireless communication between the robot and home hub, which meant there was no data from the home’s smart sensors. A fourth reason (c) is due to two factors (i) Rose had forgotten to put on her fall alarm bracelet, but (ii) even if she had been wearing it the bracelet would have been ineffective as it too communicates with the robot via the home hub. The failure of communication between the robot and home hub is particularly serious because, as the graph shows, even if the first two pathways (a) and (b) to the robot’s fall detection system had operated correctly the robot would still not have been able to raise the alarm, indicated by path (e). To use the Swiss cheese metaphor from section 2, over reliance on communication with the home hub represents a set of holes in the layers of safety which all line up.\\n The key conclusions from this analysis are that (i) the robot did not cause Rose’s accident, (ii) the robot failed to raise the alarm following Rose’s fall – one of its primary safeguarding functions, and (iii) failures and shortcomings of the smart home’s infrastructure contributed significantly to the robot’s failure. The robot’s\\n Robot Accident Investigation\\n\\n Fig. 3 Why-because graph for Rose’s accident\\n failure might have had very severe consequences had a neighbour not called upon Rose and raised the alarm.\\n As a consequence of their investigation the team are able to to make a set of recommendations to prevent similar accidents happening in future. These recom- mendations are, in order of priority:\\n 1. Equip the robot with a backup communications system, in case the WiFi fails.\\nA recommended approach might, for instance, be to integrate a module allowing the robot to send text or data messages via a 3G GSM connection to the public cellular network.\\n 2. Equally important is that if the robot detects a WiFi connectivity failure it should not be alerting its user (Rose) but instead sending an alert to a maintenance engi- neer via its backup communication system.\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n 3. Equip the home hub with the ability to send an emergency call directly – via a landline for instance – when the fall bracelet is triggered, so that this particular alarm is not routed via the robot.\\n 4. Improve the sensitivity of the robot’s microphones to increase their range.\\n5. Add a new function to the robot so that it reminds Rose to put on her fall bracelet\\n every day.\\n 6. Advise the cleaner not to use disinfectants on the robot.\\n 6 Concluding discussion\\n RoboTIPS\\n The work of this chapter is part of five-year programme RoboTIPS: Responsible Robots for the Digital Economy16. RoboTIPS has several themes, two of which are of relevance to this paper. The first is the technical specification and prototyping of the EBB, including the development of model EBBs which will be used as the basis of implementations and trials by project industrial partners. There will be two model EBBs, one a hardware implementation and the other a software module, and their specifications, code and designs will be published as open source in order to support and encourage others to build EBBs into their robots. Ultimately we would like to see industry standards emerge for EBBs, noting that we would need separate specifications for different domains: one EBB standard for AVs, another for healthcare robots, a third for robot toys/educational robots, and so on.\\n Second we are designing and running three staged (mock) accidents, and we anticipate one in the application domain of assisted living robots, one in educational (toy) robots, and another for autonomous vehicles. We believe this to be the world’s first experimental investigation of accident investigation in the field of social robots.\\nFor each of these staged scenarios we will be using real robots and will invite human volunteers to act in three roles, as\\n 1. subject(s) of the accident, 2. witnesses to the accident, and as 3. members of the accident investigation team.\\n One key aim of these staged accidents is to trial, develop and refine the framework for social robot accident investigation outlined in this paper.\\n Thus we aim to develop and demonstrate both technologies and processes (and ultimately policy recommendations) for effective social robot accident investiga- tion. And as the whole project is conducted within the framework of Responsible Research and Innovation it is a case study in Responsible Robotics.\\n 16 https://www.robotips.co.uk/\\n Robot Accident Investigation\\n The Bigger Picture\\n\\n There are two important details that we omitted from the accident scenario outlined in section 1, then developed in section 5.3. The first is who needs to be on a robot ac- cident investigation team. And the second – and perhaps more fundamental question – who do you call upon to investigate a social robot accident?\\n Concerning the makeup of a social robot accident investigation team, if we fol- low best practice it would be a multi-disciplinary team. One report, for instance, described multi-disciplinary teams formed to investigate sleep related fatal vehi- cle accidents as “consisting of a police officer, a road engineer, a traffic engineer, a physician, and in certain cases a psychologist” [32]. Such teams did not require the involvement of vehicle manufacturers, but more recent fatal accidents involving AVs have needed to engage the manufacturer, to provide both expertise on the AV’s autopilot technology and access to essential data from the vehicle’s proprietary data logger [4]. Robot accident investigations will similarly need to call upon the assis- tance of robot manufacturers, both to provide data logs and advice on the robot’s operation. We would therefore expect social robot accident investigation teams to consist of (i) an independent lead investigator with experience of accident investi- gation, (ii) an independent expert on human-robot interaction, (iii) an independent expert on robot hardware and software, (iv) a senior manager from the environment in which the accident took place, and (v) one of the robot manufacturer’s senior en- gineers. Depending on the context of the accident the team might additionally need, for instance, a (child-)psychologist or senior health-care specialist.\\n Consider now the question: who do you call when there has been a robot ac- cident?17 At present there is no social robot equivalent of the UK Air Accident Investigations Branch18, or Healthcare Safety Investigation Branch (HSIB)19. A se- rious AV accident would of course be attended by a police road traffic accident unit, although they would almost certainly encounter difficulties getting to the bottom of failures of the vehicle’s autopilot AI. The US National Transport Safety Board20 (NTSB) is the only investigation branch known to have experience of AV accidents, having investigated five to date (it is notable that the NTSB is the same agency responsible for air accident investigation in the US, and thus able to bring that con- siderable experience to bear on AV accidents).\\n For assisted living robots deployed in a care home settings, such as in our ex- ample scenario, accidents could be reported to both the Care Quality Commission21 (CQC) – the regulator of health and social care in England – and/or the Health and Safety Executive22 (HSE), since care homes are also workplaces. Again it is\\n 17 After the paramedics, that is.\\n\\n https://www.gov.uk/government/organisations/\\n air-accidents-investigation-branch 19 https://www.hsib.org.uk/ 20 https://www.ntsb.gov/Pages/default.aspx 21 https://www.cqc.org.uk/ 22 https://www.hse.gov.uk/\\n\\n Winfield, Winkle, Webb, Lyngs, Jirotka and Macrae\\n doubtful that either the CQC or HSE would have the expertise needed to investi- gate accidents involving robots. Accidents involving workplace assistant robots, or robots used in schools – including near misses – would certainly need to be reported to the HSE. It is clear that as the use of social robots in society increases, regulators such as the CQC and HSE will need to create robot accident investigation branches, as would the HSIB for surgical or healthcare robots. Even more urgent is the need to record all such accidents – again including near misses – so that we have, at the least, statistics on the number and type of such accidents.\\n Until such mechanisms exist, or for robot accidents in settings that fall outside those outlined here, the only recourse we would have is to contact the robot’s man- ufacturer, thus underlining the importance of clear labelling of the robots make and model alongside contact details for the manufacturer of the robot itself23. Even if the robot and its manufacturer does not yet have data logging technologies (such as the EBB) or processes for accident investigation in place, we would hope that they would take accidents seriously. A responsible manufacturer would both investigate the accident – drawing in outside expertise where needed – and effect remedies to correct faults. Ideally social robot manufacturers would adopt the data sharing phi- losophy that has proven so effective in aviation safety, summed up by the motto “anybodys accident is everybodys accident”.\\n Acknowledgements The work of this chapter has been conducted within EPSRC project Robo- TIPS, grant reference EP/S',\n",
       " 'metadata': {'topic': 'Applied AI',\n",
       "  'keyword': 'robotics',\n",
       "  'filename': '2005.07474v1.pdf'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_documents[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
